{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the last exercise we finished with a simple single layer MLP with softmax for multiclass classification. In this session we will design deep models and observe their performance. To do this, we will use the [Keras API](https://keras.io/) with tensorflow, which will allow us to design, train and evaluate simple models much faster and with fewer lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "from utils import plot_samples, plot_curves\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# force random seed for results to be reproducible\n",
    "SEED = 4242\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before building deep networks, let's first train the same architecture as the one in the previous exercise, this time using keras. \n",
    "\n",
    "Let's begin by loading the MNIST dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# Display some of the samples\n",
    "plot_samples(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We flatten and normalize images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Categories need to be converted to one-hot vectors for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "y_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's write the model from the previous exercise in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "# in the first layer we need to specify the input shape\n",
    "model.add(Dense(10, input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: ```model.summary()``` gave us the total number of trainable parameters of our model. How is this number obtained? Manually calculate the parameters of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now ready to train. Let's define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "lr = 0.01\n",
    "# For now we will not decrease the learning rate\n",
    "decay = 0\n",
    "\n",
    "optim = SGD(lr=lr, decay=decay, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In Keras, we need to compile the model to define the loss and the optimizer we want to use. Since we are dealing with a classification problem, we will use the cross entropy loss, which is already defined in keras. Additionally, we will incorporate the accuracy as an additional metric to compute at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's train the model. ```model.fit()``` will do the training loop for us. We just need to pass the training data ```X_train``` and labels ```Y_train``` as input, specify the ```batch_size``` and the number of epochs ```nb_epoch``` we want to do. We also pass the test set ```(X_test,Y_Test)``` as validation data, which will allow us to see how the model performs on the test data as training progresses. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_epoch = 30\n",
    "verbose = 2\n",
    "\n",
    "t = time.time()\n",
    "# 30 seconds for 20 epochs on GeForce GTX 980\n",
    "history = model.fit(X_train, Y_train,\n",
    "                batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                verbose=verbose,validation_data=(X_test, Y_test))\n",
    "\n",
    "print (time.time() - t, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can plot the loss and accuracy curves with the ```history``` object returned by ```model.fit()```. The function ````plot_curves```, which is defined in ```utils.py``` will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_curves(history,nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The curve trend indicates that the model may be able to improve if we train it for longer, but for now let's leave it here.\n",
    "\n",
    "Let's now evaluate our model. ```model.evaluate()``` will take all the test samples, forward them through the network and return the average loss, and any additional metrics we specified (in our case, the accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print (\"Loss: %f\"%(score[0]))\n",
    "print (\"Accuracy: %f\"%(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We reach an accuracy of 92%, which is similar to the one we obtained in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try to train a model with a hidden layer between the input and the classifier. \n",
    "\n",
    "**Exercise**: Modify the previous architecture to include this layer with 128 neurons. Make sure to use a non-linearity between the two layers (e.g. ReLU). Also remember that, in keras, the ```input_shape``` must be passed to the first layer of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# MODEL DEFINITION\n",
    "model = Sequential()\n",
    "\n",
    "# TODO: add layers to this network as indicated in the exercise\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Compute the number of parameters and check if they match the ones given by ```model.summary()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's train the model you just defined. Notice that the compilation and training code did not change at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# COMPILE & TRAIN\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "t = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                verbose=2,validation_data=(X_test, Y_test))\n",
    "print (time.time() - t, \"seconds.\")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print (\"-\"*10)\n",
    "print (\"Loss: %f\"%(score[0]))\n",
    "print (\"Accuracy: %f\"%(score[1]))\n",
    "plot_curves(history,nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, adding a hidden layer pushed the accuracy from 0.92 to almost 0.98. Of course the number of parameters has increased significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Going deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Can we keep adding layers just like that and get better and better accuracy? \n",
    "\n",
    "**Exercise**: Design and train network with 6 hidden layers with 128 neurons + classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "model = Sequential()\n",
    "# TODO: add layers as indicated\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "t = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                verbose=2,validation_data=(X_test, Y_test))\n",
    "print (time.time() - t, \"seconds.\")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print (\"-\"*10)\n",
    "print (\"Loss: %f\"%(score[0]))\n",
    "print (\"Accuracy: %f\"%(score[1]))\n",
    "plot_curves(history,nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Turns out that adding more layers did not do much for us. Accuracy is pretty much the same as the one we obtained with a single hidden layer, and now our model is overfitting to training data. How can we fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Add dropout layer(s) to the model and see how their effect in the training curves & accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "dratio = 0.2\n",
    "H_DIM = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# TODO: add layers to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['accuracy'])\n",
    "t = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                verbose=verbose,validation_data=(X_test, Y_test))\n",
    "print (time.time() - t, \"seconds.\")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print (\"-\"*10)\n",
    "print (\"Loss: %f\"%(score[0]))\n",
    "print (\"Accuracy: %f\"%(score[1]))\n",
    "plot_curves(history,nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Additional exercises:\n",
    "\n",
    "- Use one of the networks to study the effect of the following components:\n",
    "    - learning rate\n",
    "    - optimizer (you can find a list of the different optimizers available in keras [here](https://keras.io/optimizers/))\n",
    "    - learning rate decay\n",
    "- Try adding weight regularization in the hidden layers of the network. You can check how to do this in the documentation for [Dense layers](https://keras.io/layers/core/) and [Regularizers](https://keras.io/regularizers/) in keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
