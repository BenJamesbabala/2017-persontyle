{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Neural Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this session we will use the approach described in [this paper](https://arxiv.org/abs/1508.06576) to apply a particular style to a content image by means of optimization using the weights of a pretrained convnet (VGG16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "from keras.applications import vgg16\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "from style import * \n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's first display the images we chose. We want to transfer the style of the second image into the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_image_path = '../images/neural_style/source/tubingen.jpg'\n",
    "style_reference_image_path = '../images/neural_style/style/starry_night.jpg'\n",
    "\n",
    "f, axarr = plt.subplots(1,2,figsize=(20,20))\n",
    "axarr[0].imshow(imread(base_image_path))\n",
    "axarr[0].axis('off')\n",
    "axarr[1].imshow(imread(style_reference_image_path))\n",
    "axarr[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We load both images and create tensor representations of the appropriate size. We also define the ```input_tensor``` which will be the input that VGG expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path,img_nrows,img_ncols))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path,img_nrows,img_ncols))\n",
    "\n",
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
    "\n",
    "# this is the tensor that we will use as input to VGG (the content, style, and the combination images concatenated)\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load VGG and get a dictionary of all its layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we choose the layers of the network that we want to use to represent the style and the content. You can play with different combinations if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# content layers\n",
    "content_layers = ['block4_conv2']\n",
    "\n",
    "# style layers\n",
    "style_layers = ['block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1',\n",
    "                'block5_conv1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we define the loss as the combination of the style and content losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"figs/style-equation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "style_weight = 0.5\n",
    "content_weight = 0.5\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "\n",
    "# loss for content layers\n",
    "for layer_name in content_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "\n",
    "    # we get the content features from the content image and the combination image\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "\n",
    "    # content_loss is defined in style.py\n",
    "    cl = content_loss(base_image_features,combination_features)\n",
    "    loss += (content_weight / len(content_layers)) * cl\n",
    "\n",
    "# loss for style layers\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    \n",
    "    # get style features from style and combination images\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    \n",
    "    # style_loss is defined in style.py\n",
    "    sl = style_loss(style_reference_features, combination_features,img_nrows,img_ncols)\n",
    "    loss += (style_weight / len(style_layers)) * sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will also include a third loss term, which encourages spatial smoothness in the generated image. Although this was not included in the original paper, it often improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_variation_weight = 1.0\n",
    "# total_variation_loss is defined in style.py - preserves local coherence\n",
    "loss += total_variation_weight*total_variation_loss(combination_image,img_nrows,img_ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we get the function that computes the gradient of the combination image with respect to the total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "# final function that will give the gradients of the generated image wrt the loss\n",
    "f_outputs = K.function([combination_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And now we iterate. At each iteration, the combination image ```x``` will be updated based on the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# the Evaluator class is defined in style.py\n",
    "evaluator = Evaluator(img_nrows,img_ncols,f_outputs)\n",
    "\n",
    "ims = []\n",
    "iterations = 10\n",
    "\n",
    "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "\n",
    "# we start with random image - this will be updated to be the combination of content and style images\n",
    "x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128.\n",
    "\n",
    "for i in range(iterations):    \n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    \n",
    "    print(i,'Current loss value:', min_val)\n",
    "    # deprocess_image is defined in style.py\n",
    "    img = deprocess_image(x.copy(),img_nrows,img_ncols)\n",
    "    \n",
    "    # append image to display later\n",
    "    ims.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will plot the images in the last 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, len(ims[:5]),figsize=(20,20))\n",
    "\n",
    "for i,im in enumerate(ims[:5]):\n",
    "    axarr[i].imshow(im)\n",
    "    axarr[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We display the last image with higher resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) \n",
    "plt.imshow(ims[-1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** You can find more source and style images in the same folders as the ones we used in the example. Try with other images and see what happens. You can also experiment with different layers in the network, different weights, or different number of iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
