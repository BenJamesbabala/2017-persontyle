{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM character language model\n",
    "\n",
    "In this notebook we are going to proof the effectiveness of Recurrent Neural Networks, and more specifically Long Short Term Memory (LSTM) RNNs, to generate sequences of characters out of some text samples we show it. \n",
    "\n",
    "Keras will be the library used to do so for its simplicity in defining the model and training structure, following their [`lstm_text_generation.py`](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py) official example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The example trains on Nietzsche textual samples, such that the LSTM will learn about the style of this author in writing the generated sentences. The dataset is easily found in Amazon S3 service publicly. We download it (if required) and load it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "# First, the Nietzsche corpus is downloaded from Amazon S3 database\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The mapping dictionaries must be built: \n",
    "\n",
    "* char2idx: for an input char, assign an integer signaling the active index in the one-hot code\n",
    "* idx2char: does the reverse mapping to translate output predictions from network to chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The dictionary mapping characters to one-hot indices is built\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "# we keep an idx2char dict too to convert what the network predicts into characters during sampling\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('total chars/one-hot length:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now text must be chopped into sequences of maxlen characters. Maxlen will be the truncated size of backprop through time. The sequences are built from semi-redundant strings of chars, for example if we have the sentence \"the cat sat on the mat\" with `step=3` and `maxlen=6`:\n",
    "\n",
    "* x1 = ['the ca'] -->  y1 = 't'\n",
    "* x2 = [' cat s'] -->  y2 = 'a'\n",
    "* x3 = ['t sat '] -->  y3 = 'o'\n",
    "* x4 = ['at on '] -->  y4 = 't'\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this means we will backprop in time through 40 time-steps, so to generate the data we pass a sliding\n",
    "# window through the text in 3-by-3 char steps. Out of that we create the 3-D tensor input to the LSTM\n",
    "# and its output representations containing the next character after the 40 timesteps.\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now all text has to be vectorized, such that all characters must be converted to one-hot indices:\n",
    "\n",
    "* X becomes a 3-D tensor: `(num_chops, maxlen, char_vocab_size)` \n",
    "* Y becomes a 2-D tensor: `(num_chops, char_vocab_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Finally, the text is vectorized (i.e. every character is converted to a one-hot index)\n",
    "print('Vectorization...')\n",
    "beg_t = timeit.default_timer()\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char2idx[char]] = 1\n",
    "    y[i, char2idx[next_chars[i]]] = 1\n",
    "end_t = timeit.default_timer()\n",
    "print('Vectorization done in {} s'.format(end_t - beg_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "**Exercise:** The RNN model for char generation has to be defined now. Based on the documentation for https://keras.io/layers/recurrent/, use either a GRU or LSTM architecture to run the training and prediction of char streams. The model has to be compiled as well, selecting the right loss function for classification task and the optimizer to train efficiently. Advice: Use `Sequential` model for its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build the model: a single LSTM layer with a fully connected softmax output to classify which char is next\n",
    "print('Building model...')\n",
    "beg_t = timeit.default_timer()\n",
    "\n",
    "# TODO: Define the model here\n",
    "\n",
    "# TODO: Define its compilation\n",
    "\n",
    "end_t = timeit.default_timer()\n",
    "print('Elapsed time creating & compiling model: {} s'.format(end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's check the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Build a function to compute the number of parameters inside an LSTM cell and a Dense (or Fully Connected) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: define the num_lstm_params(input_dim, num_cells) function\n",
    "\n",
    "# TODO: define the num_fc_params(input_dim, num_neurons) function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: The defined funcitons are called to confirm the Keras summary parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make a sampler to set up a temperature and thus gain more variability in the output response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array with a certain temperature factor.\n",
    "    # The higher the temperature, the higher the output variability of predictions (it makes them more noisy)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_losses = []\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Training on epoch {}...'.format(iteration))\n",
    "\n",
    "    his = model.fit(X, y, batch_size=700, nb_epoch=1, verbose=0)\n",
    "    tr_losses.append(his.history['loss'])\n",
    "    print('Iteration: {}, tr loss: {}'.format(iteration, tr_losses[-1]))\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char2idx[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = idx2char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import maptlotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Finally let's check out the learning curve\n",
    "plt.plot(tr_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
