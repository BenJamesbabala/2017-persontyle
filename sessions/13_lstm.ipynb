{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM character language model\n",
    "\n",
    "In this notebook we are going to proof the effectiveness of Recurrent Neural Networks, and more specifically Long Short Term Memory (LSTM) RNNs, to generate sequences of characters out of some text samples we show it. \n",
    "\n",
    "Keras will be the library used to do so for its simplicity in defining the model and training structure, following their [`lstm_text_generation.py`](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py) official example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The example trains on Nietzsche textual samples, such that the LSTM will learn about the style of this author in writing the generated sentences. The dataset is easily found in Amazon S3 service publicly. We download it (if required) and load it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "# First, the Nietzsche corpus is downloaded from Amazon S3 database\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The mapping dictionaries must be built: \n",
    "\n",
    "* char2idx: for an input char, assign an integer signaling the active index in the one-hot code\n",
    "* idx2char: does the reverse mapping to translate output predictions from network to chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars/one-hot length: 59\n"
     ]
    }
   ],
   "source": [
    "# The dictionary mapping characters to one-hot indices is built\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = dict((c, i) for i, c in enumerate(chars))\n",
    "# we keep an idx2char dict too to convert what the network predicts into characters during sampling\n",
    "idx2char = dict((i, c) for i, c in enumerate(chars))\n",
    "print('total chars/one-hot length:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now text must be chopped into sequences of maxlen characters. Maxlen will be the truncated size of backprop through time. The sequences are built from semi-redundant strings of chars, for example if we have the sentence \"the cat sat on the mat\" with `step=3` and `maxlen=6`:\n",
    "\n",
    "* x1 = ['the ca'] -->  y1 = 't'\n",
    "* x2 = [' cat s'] -->  y2 = 'a'\n",
    "* x3 = ['t sat '] -->  y3 = 'o'\n",
    "* x4 = ['at on '] -->  y4 = 't'\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200287\n"
     ]
    }
   ],
   "source": [
    "# this means we will backprop in time through 40 time-steps, so to generate the data we pass a sliding\n",
    "# window through the text in 3-by-3 char steps. Out of that we create the 3-D tensor input to the LSTM\n",
    "# and its output representations containing the next character after the 40 timesteps.\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now all text has to be vectorized, such that all characters must be converted to one-hot indices:\n",
    "\n",
    "* X becomes a 3-D tensor: `(num_chops, maxlen, char_vocab_size)` \n",
    "* Y becomes a 2-D tensor: `(num_chops, char_vocab_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Vectorization done in 2.20662498474 s\n"
     ]
    }
   ],
   "source": [
    "# Finally, the text is vectorized (i.e. every character is converted to a one-hot index)\n",
    "print('Vectorization...')\n",
    "beg_t = timeit.default_timer()\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char2idx[char]] = 1\n",
    "    y[i, char2idx[next_chars[i]]] = 1\n",
    "end_t = timeit.default_timer()\n",
    "print('Vectorization done in {} s'.format(end_t - beg_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "**Exercise:** The RNN model for char generation has to be defined now. Based on the documentation for https://keras.io/layers/recurrent/, use either a GRU or LSTM architecture to run the training and prediction of char streams. The model has to be compiled as well, selecting the right loss function for classification task and the optimizer to train efficiently. Advice: Use `Sequential` model for its simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Elapsed time creating & compiling model: 0.221600055695 s\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM layer with a fully connected softmax output to classify which char is next\n",
    "print('Building model...')\n",
    "beg_t = timeit.default_timer()\n",
    "\n",
    "# TODO: Define the model here\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# TODO: Define its compilation\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "end_t = timeit.default_timer()\n",
    "print('Elapsed time creating & compiling model: {} s'.format(end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 128)           96256       lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           16512       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 59)            7611        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 120,379\n",
      "Trainable params: 120,379\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's check the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Build a function to compute the number of parameters inside an LSTM cell and a Dense (or Fully Connected) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: define the num_lstm_params(input_dim, num_cells) function\n",
    "def num_lstm_params(input_dim, num_cells):\n",
    "    return 4 * (input_dim * num_cells + num_cells * num_cells + num_cells)\n",
    "# TODO: define the num_fc_params(input_dim, num_neurons) function\n",
    "def num_fc_params(input_dim, num_neurons):\n",
    "    return input_dim * num_neurons + num_neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96256\n",
      "16512\n",
      "7611\n"
     ]
    }
   ],
   "source": [
    "# TODO: The defined funcitons are called to confirm the Keras summary parameters\n",
    "print(num_lstm_params(len(chars), 128))\n",
    "print(num_fc_params(128, 128))\n",
    "print(num_fc_params(128, len(chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make a sampler to set up a temperature and thus gain more variability in the output response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array with a certain temperature factor.\n",
    "    # The higher the temperature, the higher the output variability of predictions (it makes them more noisy)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 1...\n",
      "Iteration: 1, tr loss: [2.8436239122296287]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"m when you complain about it. he goes ba\"\n",
      "m when you complain about it. he goes bant the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the on the the the the the the the the the the the the the the the the the th\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"m when you complain about it. he goes ba\"\n",
      "m when you complain about it. he goes band the the thent and thithe tinn and the\n",
      "the tufre or in ante the the want thes ton the the lof sither mere an on inting and whe und the ther thit the the the the tothe merenthe the the be sat of and the the the the the the the tor the ther ind che as, sons one\n",
      "the be thend thare the oo the of wh sher thes the le the the sere serore sref nol the of fhe ant the the an foruro spins berle the ans the\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"m when you complain about it. he goes ba\"\n",
      "m when you complain about it. he goes bagand, ume kaus whif werrinululjrcond oe lnthen. sucates, co vor fhop cdtedla\n",
      "le-, rusinsd seshed\n",
      "onorucod\n",
      "a. dos rhal, mpy a\n",
      "ninte- onn in icnefsed mnvony musccesvsosd tisvangisma gorteti\n",
      "ley fmeluslecsd,ether\" okel iln \n",
      "eutos eminn iund bfantthinl the bll\n",
      "vel\"od outucs naf ovso, is \"oftpshthes ufit ey pheroltiny ok tins inden hsor ilmifiris co estan s tasd ua cmet whe tioritisy the themseinns hel\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"m when you complain about it. he goes ba\"\n",
      "m when you complain about it. he goes balreraded,fs akefh0ns phorgwe yis resord\n",
      "ecesas ane horn:ocrevals agslbury buthe, f dniren senpidm-ey per-\n",
      "mink bes, psire\n",
      "lqereci thecs ,ur] tod of muuns or\n",
      "qtilad uvairmton, ,ers root beslionlk\n",
      "nllasv toruagl�to ateup\n",
      "ven, on twthnl ocod cnlo\n",
      "sy cetin ll\"fevlceyndgs-ulmamlfgfohy sho hireid tfer bft tot heryrnd celthott ibe, tr,, pathe tofes, ondt=ensthrgao\n",
      "to\n",
      "inicsad by; tht mo, potretel\"thil\n",
      "os \n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 2...\n",
      "Iteration: 2, tr loss: [2.3382843144373142]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"to the fact that we have meanwhile \"refo\"\n",
      "to the fact that we have meanwhile \"refore the the the and of the and and and and and and and of the the the the the the seresting and and and the the the the the and of and and and and and and and and and of and and and and and and and and and and and on the and and the and and of and and and and and and and and and and anderithe he pores and and and and and and the the and of and and and and the the the the and and and and inting the \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"to the fact that we have meanwhile \"refo\"\n",
      "to the fact that we have meanwhile \"refond wher duct re coan the indingent enderith whe os the there and of in the porisone ur in the poraprencongiting as andes in the hemance the he momerres, sichore to sole of chire fos bronongalien thid the and of the severinand the pirlane and ingente and ince anderestiat of the and the morenaly and andernenalion and in oulse at of lomene wint rentare of sher the sistouchine the sand and fores oprea\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"to the fact that we have meanwhile \"refo\"\n",
      "to the fact that we have meanwhile \"refoan euf enmeencend dalse\n",
      "of loselite's nof fpreith forcowed an merecaser\" tha hes to rancofresond cuneered in onco pimtiue ur and\n",
      "oas nopealy an cornored is moseuse\n",
      "whance in uf ruce as onasevees arthinngoxerrably orretselan\n",
      "jer. as lowwy everde oratinus. attire, fur coveroveesonaly atsounttiwipleguth of annle enbelpan\" in apncelefen themprenent ind. oo mangeuly\n",
      "and oor of eur, aud ad franie fomnto\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"to the fact that we have meanwhile \"refo\"\n",
      "to the fact that we have meanwhile \"reforen as ibisade herodesysod\n",
      "ssenge chese pereofhurjitl, toing-\"eise seititre,,, urp fooshe feuwtyr..! hes anserd veongiesor\"\n",
      "ow af umut.e\" gahh uginisomsyidtible\n",
      "\" ins lamn ok,, as orcepluingico and varpeman\n",
      "vosoo\n",
      "nopcative [atiett uocros=raliting inthy theoteulh an retseuo ful aveded as relcepteas optagest \n",
      "thar ffers rededinese sodipooftiuve ofre untoritht 9ve at angeles bos\n",
      "isterithe acpoinesof:\n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 3...\n",
      "Iteration: 3, tr loss: [2.1668623596742451]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" of]\n",
      "conduct nor his [particular] acts. \"\n",
      " of]\n",
      "conduct nor his [particular] acts. the sore and and the the seres and and and in the to the and in the preane to the indere to the and and and and and and inderes of the and in the sere and in the roull in the were a dere to the beer and of the more and to the the the is and and and and and and of the and and and and and and and and in the inderest and and and in the the more to the been is in the recerse to the is to the more to t\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" of]\n",
      "conduct nor his [particular] acts. \"\n",
      " of]\n",
      "conduct nor his [particular] acts. as and the ertion, a mome erofly and of the prition the soroof in be seing to is protient he the mongation, ard and endare, be beery the erouch of is of there coment of mane has in is deally bo the sorlly of a dines he whe wion sore a surl and in in the for ather of rewicity of the soult, in it loy sherestion the are to saliof and and and in in in in on the gonceron of is and is soul sere and ald \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" of]\n",
      "conduct nor his [particular] acts. \"\n",
      " of]\n",
      "conduct nor his [particular] acts. sorithot the deeno\n",
      "of treing..\n",
      "202\n",
      "=\"\n",
      "ouch codenail outroms, sotibe, of the ippeithes, auf we caplemat\"; of the\n",
      "reato--im, wo thouluge fral fyeis wothy rilg is \"of thie mave of of mor ive hars a bo hidicuse apt and fhat is, pritoly yomel[legy of thercutte hind alse it thaty bom to ar storad, oquingere ta a ricer, withles?--and abteen, the folly\n",
      "obse pwinkils, \"fsuligio' yupertince\n",
      "ot thellte, oud \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" of]\n",
      "conduct nor his [particular] acts. \"\n",
      " of]\n",
      "conduct nor his [particular] acts. ald thatitrefsing?--form of to colvdy mingrse\n",
      "gore stal twed nowd\n",
      "beavewy pencerthoulaty enceledces, of ffey race kickonsiot, phey us im, inagillifrrey a daxpilmd and teich.--aver theriges[s.\n",
      "33\n",
      "a\n",
      "ciwith furkes: inecru\" of tivo[mhe=doan of mhireg to ka secosonod, if vor\n",
      "xact; ly chmomtiof maen of the gevprsiobkis, fomieate sibcsiod brewe basce ove iss-hlovivligalied, thill hedseranes\n",
      ":ickinl, roat\n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 4...\n",
      "Iteration: 4, tr loss: [2.0541241986222536]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" at the awkward incapacity of the\n",
      "noble \"\n",
      " at the awkward incapacity of the\n",
      "noble in the preister and and in the mare the serting the sencess and the susting and the senterse and in the soment of the man is the soment of the seres the sertion and and in the the senter of the sender and and is the meres and the man the senting the senting in the senter the man in the man the is prenesting in the man the some the sence in the presest of the sere the preation the sencition the sur\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" at the awkward incapacity of the\n",
      "noble \"\n",
      " at the awkward incapacity of the\n",
      "noble and pration the man it must entings not but is us the concess atse hat in the restencting of the mentiches and man in a surto store the one sall the noce him the sulley of the ast a more the which sume preing and merer of revery his is conderss\n",
      "phis corting of there and it panding in whe a sellicalice the nothing that in wat deress and groce the be\n",
      "wikl of the sound of the wering and be the heresp\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" at the awkward incapacity of the\n",
      "noble \"\n",
      " at the awkward incapacity of the\n",
      "noble encommeon there\n",
      "phars in bust blowh; lage\n",
      "geiraly beervlrysialicansint and conccindy, who thes eviciouse, a whive har which be enoom't- be can stakizy an inkips)evrey theive fat\n",
      "revory that ture be s.\n",
      "\n",
      "=3 cheels ant, menes\n",
      "furndamess of mitter, a butalit poresut ono ghe neas. huchils\")\n",
      "apmonty,\"--whotuwhysy inmoring ivad havey fel spmirr; of poumuld cfetsince, wham dece disf-the hermulise, thach, \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" at the awkward incapacity of the\n",
      "noble \"\n",
      " at the awkward incapacity of the\n",
      "noble pwere geroos tin cllyset\n",
      "sammcaly magally\n",
      "crpbeissimentincaphmeut bus the nist\n",
      "cablinition acs.\n",
      "115.\n",
      "\n",
      "my a netnciust ta to whin man dincouity of precerafe cont, whom ae the ladione ins us munkiplturaeve, whe pifeduce,\" our the asst enconcintiocsenchsopenied an srlugliuncciedy on igpevorligncsaunor it\n",
      "eals on meren, whin vase-tmen [adve soriluede in pyedysoly scenthe the snegtizably makqujuod ther\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 5...\n",
      "Iteration: 5, tr loss: [1.9735829284193787]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"rching (to the number of whom has\n",
      "lately\"\n",
      "rching (to the number of whom has\n",
      "lately and and and which shan and the prestion and in a will the some the sore the soment of the sored and the sorsent and is the sourth the sore and the greation of the sore the sore the man the and the man of the resting of the sorst the sore that the sorest and and insting the sonest and and inself of the sourd and and the sore the songer of the has in a some has in the songer of the soment of the su\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"rching (to the number of whom has\n",
      "lately\"\n",
      "rching (to the number of whom has\n",
      "lately and respless and heredes, that in the lasting and that it the forling of the\n",
      "sention and in is the fering, the wall stang of the prenition and and whech be oust neture erfand in and the arlaty as or sother of the concestation in the sent and sencesing whow not the grough a cherence which a san a been a dence the selother dowe in the host love in the suplester it understing of shing the mearines, \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"rching (to the number of whom has\n",
      "lately\"\n",
      "rching (to the number of whom has\n",
      "lately of as abfoull or a hasele the as bat\n",
      "meven. the pesisthable. in scadcence a hand\n",
      "instuyser and well atration thingingy the kersole werlough compousty\n",
      "onvears of verphiganit is\n",
      "not the will grul, the sea; phaltompething hope soult of\n",
      "menaccel, as wich mare theiffuants hishence inst besturl to hiver cactaratias?  [o what toebers,\n",
      "besurned, opherstisite\".=- a mow doo the fan elding, the thy leffict \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"rching (to the number of whom has\n",
      "lately\"\n",
      "rching (to the number of whom has\n",
      "lately sracople verrascess samunys pracat ly id�mont whachsefred seathemitus couly son antos howertod---han offent:\n",
      "aquement in! icpermant\n",
      "acprefuls of locuavis. the suatian ofe for lid geraly\n",
      ".ito[be\" howh!r,\"\n",
      "s un aring! ind\n",
      "frenksand that that\n",
      "mishung exiralrede\" as for cospabbe wir batetain of\n",
      "ondeugod-cbet uthing vifuse anspoc8: thit\n",
      "orngie all wighe seen-agune inte thow, whis\n",
      "soudte. budink condin\n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 6...\n",
      "Iteration: 6, tr loss: [1.9154472180396862]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"d in the highest instance, a seat in\n",
      "par\"\n",
      "d in the highest instance, a seat in\n",
      "parte the are the preation of the and in the for the surtent of the good the has the and the deest of the portion the grood the surtent of the world and the for the preation of the and the more the present the surtent of the and the man the the and and the for the surtent of the enting of the groust of the the receraling of the and the recination of the as a the preation of the senting of the sention\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"d in the highest instance, a seat in\n",
      "par\"\n",
      "d in the highest instance, a seat in\n",
      "partation the the areal and which so reching the haperom phisones and imponity, the would who has the all ame standity the doual and exterted of the instaction of the sure and upon the selferte the pration one and and a dile even as the portion and partious and belate of the are the bether and precostains of the grought for a seally mo all the rentient of the sustertion the pursurable\n",
      "and the fromegh\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"d in the highest instance, a seat in\n",
      "par\"\n",
      "d in the highest instance, a seat in\n",
      "pardiferoustare, of sother swevelf\n",
      "mercabse virtion\n",
      "they nuturty towe has pleytrales jume alies ho the freoms othelos of it imppate's? envinglif.\" anking, the precosceloaitise that who\n",
      "rist?\" a)ne\n",
      "fhererver of not peinhpratunels ment eccopothy of foothatieve ancifutly\n",
      "ond bedasion, we ghen\n",
      "more, back of taute, artees. a yow not secfly most with of\n",
      "penctiand rest yufs are theffece opor as\n",
      "the which th\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"d in the highest instance, a seat in\n",
      "par\"\n",
      "d in the highest instance, a seat in\n",
      "parlalsely woollusifate thege enwatifie ciand drugisine--it a not ony,\n",
      "that erlibne's of phest so alf, the vike, yot which thy knean, goo\n",
      "theply,- us ouidually mased, infienally evestathess) thire we fure\n",
      "the glighted. and to det, to of beyhind\n",
      "jugh, in\n",
      "the\"cgite of\n",
      "tren aghatescofood. in a putsos the godnt:--ke lakes and, the fanothe preounke the uttor! magat\"--and hay he rifeel kedaupteran and _awb\n",
      "\n",
      "--------------------------------------------------\n",
      "Training on epoch 7...\n",
      "Iteration: 7, tr loss: [1.8662147310197268]\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"esults,\n",
      "resisted theology, whose \"hand-m\"\n",
      "esults,\n",
      "resisted theology, whose \"hand-man the rest of the instion of the presence of the section of the sent and and in the consers and the sence and in the preation of the self and in the self and the sent in the preated the self and and in the insertion and present of the self the more the consers and in the experient of the present and and the self the sent of the self and in the sent of the some the for the sent and in the self and\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"esults,\n",
      "resisted theology, whose \"hand-m\"\n",
      "esults,\n",
      "resisted theology, whose \"hand-moring the madient "
     ]
    }
   ],
   "source": [
    "tr_losses = []\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Training on epoch {}...'.format(iteration))\n",
    "\n",
    "    his = model.fit(X, y, batch_size=700, nb_epoch=1, verbose=0)\n",
    "    tr_losses.append(his.history['loss'])\n",
    "    print('Iteration: {}, tr loss: {}'.format(iteration, tr_losses[-1]))\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char2idx[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = idx2char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import maptlotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Finally let's check out the learning curve\n",
    "plt.plot(tr_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
