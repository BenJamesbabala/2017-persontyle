{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In this notebook we will exemplify how are embeddings created and we will do some vizualization of a pre-trained embedding layer with [GloVe](http://nlp.stanford.edu/projects/glove/) weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will specify the dimensionality of our embeddings, we have options: [50, 100, 200, 300]. Depending on which we choose we will load its corresponding pre-trained GloVe matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word vectors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-79163718f999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading word vectors.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove.6B.%dd.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s word vectors.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-79163718f999>\u001b[0m in \u001b[0;36mread_glove_vectors\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcoefsm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcoefsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emb_dir = '../../data/glove'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# function that reads the contents of the downloaded embeddings\n",
    "# ref: https://github.com/jarfo/dlsl/blob/master/news20/pretrained_word_embeddings.py\n",
    "def read_glove_vectors(filename):\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    coefs = None\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if coefs is None:\n",
    "            coefs = [[0] * len(values[1:])]\n",
    "        coefs.append(values[1:])\n",
    "        embeddings_index[word] = i + 1\n",
    "    f.close()\n",
    "    coefsm = np.asarray(coefs, dtype='float32')\n",
    "    return coefsm, embeddings_index\n",
    "\n",
    "\n",
    "print('Reading word vectors.')\n",
    "embedding_matrix, word2idx = read_glove_vectors(os.path.join(emb_dir, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
    "print('Found %s word vectors.' % len(word2idx))\n",
    "\n",
    "idx2word = dict((v, k) for k,v in word2idx.iteritems())\n",
    "\n",
    "VOCAB_SIZE=len(word2idx) # Keep track of the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build an inverse mapping dict to get words back out of index predictions\n",
    "idx2word = dict((v, k) for k, v in word2idx.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# create the Tensorflow op to do the embedding operation with the pre-loaded matrix\n",
    "\n",
    "# First: make the Tensorflow weights for the embeddings matrix\n",
    "wemb_init = tf.constant(embedding_matrix)\n",
    "Wemb = tf.get_variable('Weights', initializer=wemb_init)\n",
    "\n",
    "# Declare a placeholder to inject the word indeces to be projected (one-hot vectors)\n",
    "word_in = tf.placeholder(tf.int32)\n",
    "\n",
    "# Add a lookup operator to obtain the embedding of an index\n",
    "word_emb = tf.nn.embedding_lookup(Wemb, word_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Define the cosine similarity projection of an input embedding to get the nearest embeddings to the one we inject through `nearby_word` input placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Â Normalize the embedding weights to be norm 1 for the neighbour computation (hyper sphere surface radius 1)\n",
    "nemb = tf.nn.l2_normalize(Wemb, 1)\n",
    "\n",
    "# Add the nearby computation ops to check, out of a nearby_word, which are its neighbors\n",
    "nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "nearby_emb = tf.gather(nemb, nearby_word)\n",
    "\n",
    "# TODO: define the cosine similarity operation \n",
    "nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n",
    "nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n",
    "                                     min(1000, VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word dolphin code 16350\n",
      "word dog code 2927\n",
      "word house code 167\n",
      "word barcelona code 3652\n",
      "word great code 354\n",
      "ids shape:  (5,)\n",
      "\n",
      "dolphin\n",
      "=====================================\n",
      "dolphin              1.0000\n",
      "whale                0.7471\n",
      "shark                0.7335\n",
      "turtle               0.6353\n",
      "bottlenose           0.6287\n",
      "elephant             0.6223\n",
      "whales               0.5994\n",
      "monkey               0.5933\n",
      "sharks               0.5845\n",
      "humpback             0.5782\n",
      "panda                0.5778\n",
      "turtles              0.5732\n",
      "tuna                 0.5728\n",
      "dolphins             0.5666\n",
      "crocodile            0.5658\n",
      "alligator            0.5631\n",
      "orca                 0.5618\n",
      "elephants            0.5595\n",
      "cat                  0.5568\n",
      "aquarium             0.5567\n",
      "\n",
      "dog\n",
      "=====================================\n",
      "dog                  1.0000\n",
      "cat                  0.8798\n",
      "dogs                 0.8344\n",
      "pet                  0.7450\n",
      "puppy                0.7236\n",
      "horse                0.7110\n",
      "animal               0.6817\n",
      "pig                  0.6554\n",
      "boy                  0.6545\n",
      "cats                 0.6472\n",
      "rabbit               0.6469\n",
      "goat                 0.6299\n",
      "sled                 0.6258\n",
      "monkey               0.6221\n",
      "cow                  0.6219\n",
      "rat                  0.6167\n",
      "breed                0.6090\n",
      "mad                  0.6017\n",
      "hound                0.5999\n",
      "bird                 0.5997\n",
      "\n",
      "house\n",
      "=====================================\n",
      "house                1.0000\n",
      "office               0.7582\n",
      "senate               0.7205\n",
      "room                 0.7150\n",
      "houses               0.6888\n",
      "capitol              0.6852\n",
      "building             0.6847\n",
      "home                 0.6720\n",
      "clinton              0.6707\n",
      "congressional        0.6693\n",
      "mansion              0.6651\n",
      "congress             0.6636\n",
      "floor                0.6604\n",
      "bush                 0.6594\n",
      "door                 0.6543\n",
      "once                 0.6511\n",
      "republican           0.6450\n",
      "white                0.6438\n",
      "the                  0.6434\n",
      "where                0.6394\n",
      "\n",
      "barcelona\n",
      "=====================================\n",
      "barcelona            1.0000\n",
      "madrid               0.8428\n",
      "valencia             0.8055\n",
      "sevilla              0.7542\n",
      "marseille            0.7344\n",
      "porto                0.7085\n",
      "liverpool            0.7014\n",
      "fc                   0.6964\n",
      "bayern               0.6964\n",
      "zaragoza             0.6956\n",
      "atletico             0.6923\n",
      "villarreal           0.6903\n",
      "lyon                 0.6873\n",
      "milan                0.6835\n",
      "barca                0.6823\n",
      "bilbao               0.6821\n",
      "deportivo            0.6788\n",
      "manchester           0.6733\n",
      "monaco               0.6700\n",
      "eindhoven            0.6695\n",
      "\n",
      "great\n",
      "=====================================\n",
      "great                1.0000\n",
      "greatest             0.7883\n",
      "good                 0.7593\n",
      "little               0.7586\n",
      "much                 0.7477\n",
      "well                 0.7401\n",
      "big                  0.7319\n",
      "kind                 0.7309\n",
      "important            0.7287\n",
      "there                0.7226\n",
      "way                  0.7116\n",
      "lot                  0.7113\n",
      "tremendous           0.7086\n",
      "this                 0.7080\n",
      "very                 0.7076\n",
      "perhaps              0.7075\n",
      "famous               0.7072\n",
      "brought              0.7061\n",
      "time                 0.7025\n",
      "always               0.7016\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# TODO: play around with word_examples to be projected\n",
    "word_examples = ['dolphin', 'dog', 'house', 'barcelona', 'great']\n",
    "\n",
    "# make nearby function to obtain nearby words given the list of words\n",
    "# ref: https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n",
    "def nearby(sess, ids, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    #ids = np.array([word2idx[word] for word in words])\n",
    "    print('ids shape: ', ids.shape)\n",
    "    vals, idx = sess.run([nearby_val, nearby_idx], {nearby_word:ids})\n",
    "    for i, word_idx in enumerate(ids):\n",
    "        print(\"\\n%s\\n=====================================\" % (idx2word[word_idx]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (idx2word[neighbor], distance))\n",
    "\n",
    "# Encode the words to their indices and infer the mapped word\n",
    "word_codes = []\n",
    "for word in word_examples:\n",
    "    try:\n",
    "        word_idx = word2idx[word]\n",
    "        print('word {} code {}'.format(word, word_idx))\n",
    "    except KeyError:\n",
    "        # if the word is not in the vocab, map to UNK (0)\n",
    "        print('WARNING: {} not in vocabulary'.format(word))\n",
    "        continue\n",
    "    word_codes.append(word_idx)\n",
    "    #print('word {} emb {}'.format(word, sess.run(word_emb, {word_in:word_idx})))\n",
    "\n",
    "# do the neighbor mapping\n",
    "nearby(sess, np.array(word_codes, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Build the arithmetic operations to check the semantic relationships between embeddings, for example: king - main + woman should result in queen as the closest vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Arithmetic examples\n",
    "# TODO: Make the placeholders to insert the a, b, and c vectors to do: a - b + c = ...\n",
    "analogy_a = tf.placeholder(dtype=tf.int32)\n",
    "analogy_b = tf.placeholder(dtype=tf.int32)\n",
    "analogy_c = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# TODO: Each row of a_emb, b_emb, c_emb is a word's embedding vector.\n",
    "a_emb = tf.gather(nemb, analogy_a)  # a's embs\n",
    "b_emb = tf.gather(nemb, analogy_b)  # b's embs\n",
    "c_emb = tf.gather(nemb, analogy_c)  # c's embs\n",
    "\n",
    "# We expect that d's embedding vectors on the unit hyper-sphere is\n",
    "# near: c_emb + (b_emb - a_emb)\n",
    "# TODO: target = ...\n",
    "target = c_emb + (b_emb - a_emb)\n",
    "\n",
    "# Compute cosine distance between each pair of target and vocab.\n",
    "# dist has shape [N, vocab_size].\n",
    "dist = tf.matmul(target, nemb, transpose_b=True)\n",
    "\n",
    "# For each operation (row in dist), find the top 4 words.\n",
    "_, pred_idx = tf.nn.top_k(dist, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word codes shape:  (1, 3)\n",
      "word codes:  [[17654  9324 11567]]\n",
      "walker\n",
      "ellis\n",
      "webster\n",
      "elephant\n"
     ]
    }
   ],
   "source": [
    "analogy_words = ['proton', 'elephant', 'maxwell']\n",
    "\n",
    "# convert each to an index\n",
    "word_codes = []\n",
    "for a_word in analogy_words:\n",
    "    try:\n",
    "        w_idx = word2idx[a_word]\n",
    "    except KeyError:\n",
    "        print('{} not in vocab, failure [!]'.format(a_word))\n",
    "        break\n",
    "    word_codes.append(w_idx)\n",
    "    \n",
    "word_codes = np.array(word_codes).reshape((1, -1))\n",
    "print('word codes shape: ', word_codes.shape)\n",
    "print('word codes: ', word_codes)\n",
    "idx = sess.run(pred_idx, {analogy_a: word_codes[:, 0], analogy_b: word_codes[:, 1], analogy_c: word_codes[:, 2]})\n",
    "for widx in idx[0]:\n",
    "    print(idx2word[widx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = Wemb.name\n",
    "\n",
    "train_summary_dir = 'embed_summary'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter(train_summary_dir)\n",
    "\n",
    "if not os.path.exists(train_summary_dir):\n",
    "    os.makedirs(train_summary_dir)\n",
    "\n",
    "with open( os.path.join(train_summary_dir, 'metadata.tsv') , 'w') as metadata_stream:\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        metadata_stream.write( str(idx2word[i + 1])+'\\n')\n",
    "print('Embedding metadata written to: ', os.path.join(train_summary_dir, 'metadata.tsv') )\n",
    "embedding.metadata_path = os.path.join(train_summary_dir, 'metadata.tsv')\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "saver.save(sess, os.path.join(train_summary_dir, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
