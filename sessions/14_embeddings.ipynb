{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In this notebook we will exemplify how are embeddings created and we will do some vizualization of a pre-trained embedding layer with [GloVe](http://nlp.stanford.edu/projects/glove/) weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will specify the dimensionality of our embeddings, we have options: [50, 100, 200, 300]. Depending on which we choose we will load its corresponding pre-trained GloVe matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emb_dir = '../../data/glove'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# function that reads the contents of the downloaded embeddings\n",
    "# ref: https://github.com/jarfo/dlsl/blob/master/news20/pretrained_word_embeddings.py\n",
    "def read_glove_vectors(filename):\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    coefs = None\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if coefs is None:\n",
    "            coefs = [[0] * len(values[1:])]\n",
    "        coefs.append(values[1:])\n",
    "        embeddings_index[word] = i + 1\n",
    "    f.close()\n",
    "    coefsm = np.asarray(coefs, dtype='float32')\n",
    "    return coefsm, embeddings_index\n",
    "\n",
    "\n",
    "print('Reading word vectors.')\n",
    "embedding_matrix, word2idx = read_glove_vectors(os.path.join(emb_dir, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
    "print('Found %s word vectors.' % len(word2idx))\n",
    "\n",
    "idx2word = dict((v, k) for k,v in word2idx.iteritems())\n",
    "\n",
    "VOCAB_SIZE=len(word2idx) # Keep track of the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build an inverse mapping dict to get words back out of index predictions\n",
    "idx2word = dict((v, k) for k, v in word2idx.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# create the Tensorflow op to do the embedding operation with the pre-loaded matrix\n",
    "\n",
    "# First: make the Tensorflow weights for the embeddings matrix\n",
    "wemb_init = tf.constant(embedding_matrix)\n",
    "Wemb = tf.get_variable('Weights', initializer=wemb_init)\n",
    "\n",
    "# Declare a placeholder to inject the word indeces to be projected (one-hot vectors)\n",
    "word_in = tf.placeholder(tf.int32)\n",
    "\n",
    "# Add a lookup operator to obtain the embedding of an index\n",
    "word_emb = tf.nn.embedding_lookup(Wemb, word_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Define the cosine similarity projection of an input embedding to get the nearest embeddings to the one we inject through `nearby_word` input placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Â Normalize the embedding weights to be norm 1 for the neighbour computation (hyper sphere surface radius 1)\n",
    "nemb = tf.nn.l2_normalize(Wemb, 1)\n",
    "\n",
    "# Add the nearby computation ops to check, out of a nearby_word, which are its neighbors\n",
    "nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "nearby_emb = tf.gather(nemb, nearby_word)\n",
    "\n",
    "# nearby_emb contains the embedding for our nearby-fetching vector\n",
    "\n",
    "# TODO: define the cosine similarity operation \n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# TODO: play around with word_examples to be projected\n",
    "word_examples = ['dolphin', 'dog', 'house', 'barcelona', 'great']\n",
    "\n",
    "# make nearby function to obtain nearby words given the list of words\n",
    "# ref: https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n",
    "def nearby(sess, ids, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    #ids = np.array([word2idx[word] for word in words])\n",
    "    print('ids shape: ', ids.shape)\n",
    "    vals, idx = sess.run([nearby_val, nearby_idx], {nearby_word:ids})\n",
    "    for i, word_idx in enumerate(ids):\n",
    "        print(\"\\n%s\\n=====================================\" % (idx2word[word_idx]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (idx2word[neighbor], distance))\n",
    "\n",
    "# Encode the words to their indices and infer the mapped word\n",
    "word_codes = []\n",
    "for word in word_examples:\n",
    "    try:\n",
    "        word_idx = word2idx[word]\n",
    "        print('word {} code {}'.format(word, word_idx))\n",
    "    except KeyError:\n",
    "        # if the word is not in the vocab, map to UNK (0)\n",
    "        print('WARNING: {} not in vocabulary'.format(word))\n",
    "        continue\n",
    "    word_codes.append(word_idx)\n",
    "    #print('word {} emb {}'.format(word, sess.run(word_emb, {word_in:word_idx})))\n",
    "\n",
    "# do the neighbor mapping\n",
    "nearby(sess, np.array(word_codes, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Build the arithmetic operations to check the semantic relationships between embeddings, for example: king - main + woman should result in queen as the closest vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Arithmetic examples\n",
    "# TODO: Make the placeholders to insert the a, b, and c vectors to do: a - b + c = ...\n",
    "# analogy_a = ...\n",
    "# analogy_b = ...\n",
    "# analogy_c = ...\n",
    "\n",
    "# TODO: Each row of a_emb, b_emb, c_emb is a word's embedding vector.\n",
    "# a_emb = ...\n",
    "# b_emb = ...\n",
    "# c_emb = ...\n",
    "\n",
    "# We expect that d's embedding vectors on the unit hyper-sphere is\n",
    "# near: c_emb + (b_emb - a_emb)\n",
    "# TODO: target = ...\n",
    "\n",
    "# Compute cosine distance between each pair of target and vocab.\n",
    "# dist has shape [N, vocab_size].\n",
    "dist = tf.matmul(target, nemb, transpose_b=True)\n",
    "\n",
    "# For each operation (row in dist), find the top 4 words.\n",
    "_, pred_idx = tf.nn.top_k(dist, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word codes shape:  (1, 3)\n",
      "word codes:  [[17654  9324 11567]]\n",
      "walker\n",
      "ellis\n",
      "webster\n",
      "elephant\n"
     ]
    }
   ],
   "source": [
    "analogy_words = ['proton', 'elephant', 'maxwell']\n",
    "\n",
    "# convert each to an index\n",
    "word_codes = []\n",
    "for a_word in analogy_words:\n",
    "    try:\n",
    "        w_idx = word2idx[a_word]\n",
    "    except KeyError:\n",
    "        print('{} not in vocab, failure [!]'.format(a_word))\n",
    "        break\n",
    "    word_codes.append(w_idx)\n",
    "    \n",
    "word_codes = np.array(word_codes).reshape((1, -1))\n",
    "print('word codes shape: ', word_codes.shape)\n",
    "print('word codes: ', word_codes)\n",
    "idx = sess.run(pred_idx, {analogy_a: word_codes[:, 0], analogy_b: word_codes[:, 1], analogy_c: word_codes[:, 2]})\n",
    "for widx in idx[0]:\n",
    "    print(idx2word[widx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
