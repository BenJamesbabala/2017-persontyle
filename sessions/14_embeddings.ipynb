{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In this notebook we will exemplify how are embeddings created and we will do some vizualization of a pre-trained embedding layer with [GloVe](http://nlp.stanford.edu/projects/glove/) weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will specify the dimensionality of our embeddings, we have options: [50, 100, 200, 300]. Depending on which we choose we will load its corresponding pre-trained GloVe matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emb_dir = '../../data/glove'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# function that reads the contents of the downloaded embeddings\n",
    "# ref: https://github.com/jarfo/dlsl/blob/master/news20/pretrained_word_embeddings.py\n",
    "def read_glove_vectors(filename):\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    coefs = None\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if coefs is None:\n",
    "            coefs = [[0] * len(values[1:])]\n",
    "        coefs.append(values[1:])\n",
    "        embeddings_index[word] = i + 1\n",
    "    f.close()\n",
    "    coefsm = np.asarray(coefs, dtype='float32')\n",
    "    return coefsm, embeddings_index\n",
    "\n",
    "\n",
    "print('Reading word vectors.')\n",
    "embedding_matrix, word2idx = read_glove_vectors(os.path.join(emb_dir, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
    "print('Found %s word vectors.' % len(word2idx))\n",
    "\n",
    "idx2word = dict((v, k) for k,v in word2idx.iteritems())\n",
    "\n",
    "VOCAB_SIZE=len(word2idx) # Keep track of the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build an inverse mapping dict to get words back out of index predictions\n",
    "idx2word = dict((v, k) for k, v in word2idx.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# create the Tensorflow op to do the embedding operation with the pre-loaded matrix\n",
    "\n",
    "# First: make the Tensorflow weights for the embeddings matrix\n",
    "wemb_init = tf.constant(embedding_matrix)\n",
    "Wemb = tf.get_variable('Weights', initializer=wemb_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Define the cosine similarity projection of an input embedding to get the nearest embeddings to the one we inject through `nearby_word` input placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Â Normalize the embedding weights to be norm 1 for the neighbour computation (hyper sphere surface radius 1)\n",
    "nemb = tf.nn.l2_normalize(Wemb, 1)\n",
    "\n",
    "# Add the nearby computation ops to check, out of a nearby_word, which are its neighbors\n",
    "nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# TODO: select word embedding based on index (nearby_word)\n",
    "# nearby_emb = ...\n",
    "\n",
    "\n",
    "# TODO: define the cosine similarity operation between our nearby_emb \n",
    "# nearby_dist = ...\n",
    "\n",
    "# Now select the top k words\n",
    "nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n",
    "                                     min(1000, VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# TODO: play around with word_examples to be projected\n",
    "word_examples = ['dolphin', 'dog', 'house', 'barcelona', 'great']\n",
    "\n",
    "# make nearby function to obtain nearby words given the list of words\n",
    "# ref: https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n",
    "def nearby(sess, ids, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    #ids = np.array([word2idx[word] for word in words])\n",
    "    print('ids shape: ', ids.shape)\n",
    "    vals, idx = sess.run([nearby_val, nearby_idx], {nearby_word:ids})\n",
    "    for i, word_idx in enumerate(ids):\n",
    "        print(\"\\n%s\\n=====================================\" % (idx2word[word_idx]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (idx2word[neighbor], distance))\n",
    "\n",
    "# Encode the words to their indices and infer the mapped word\n",
    "word_codes = []\n",
    "for word in word_examples:\n",
    "    try:\n",
    "        word_idx = word2idx[word]\n",
    "        print('word {} code {}'.format(word, word_idx))\n",
    "    except KeyError:\n",
    "        # if the word is not in the vocab, map to UNK (0)\n",
    "        print('WARNING: {} not in vocabulary'.format(word))\n",
    "        continue\n",
    "    word_codes.append(word_idx)\n",
    "    #print('word {} emb {}'.format(word, sess.run(word_emb, {word_in:word_idx})))\n",
    "\n",
    "# do the neighbor mapping\n",
    "nearby(sess, np.array(word_codes, dtype=np.int32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
