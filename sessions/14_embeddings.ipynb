{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In this notebook we will exemplify how are embeddings created and we will do some vizualization of a pre-trained embedding layer with [GloVe](http://nlp.stanford.edu/projects/glove/) weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will specify the dimensionality of our embeddings, we have options: [50, 100, 200, 300]. Depending on which we choose we will load its corresponding pre-trained GloVe matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "emb_dir = '../../data/glove'\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# function that reads the contents of the downloaded embeddings\n",
    "# ref: https://github.com/jarfo/dlsl/blob/master/news20/pretrained_word_embeddings.py\n",
    "def read_glove_vectors(filename):\n",
    "    embeddings_index = {}\n",
    "    f = open(filename)\n",
    "    coefs = None\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if coefs is None:\n",
    "            coefs = [[0] * len(values[1:])]\n",
    "        coefs.append(values[1:])\n",
    "        embeddings_index[word] = i + 1\n",
    "    f.close()\n",
    "    coefsm = np.asarray(coefs, dtype='float32')\n",
    "    return coefsm, embeddings_index\n",
    "\n",
    "\n",
    "print('Reading word vectors.')\n",
    "embedding_matrix, word2idx = read_glove_vectors(os.path.join(emb_dir, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
    "print('Found %s word vectors.' % len(word2idx))\n",
    "\n",
    "idx2word = dict((v, k) for k,v in word2idx.iteritems())\n",
    "\n",
    "VOCAB_SIZE=len(word2idx) # Keep track of the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build an inverse mapping dict to get words back out of index predictions\n",
    "idx2word = dict((v, k) for k, v in word2idx.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# create the Tensorflow op to do the embedding operation with the pre-loaded matrix\n",
    "\n",
    "# First: make the Tensorflow weights for the embeddings matrix\n",
    "wemb_init = tf.constant(embedding_matrix)\n",
    "Wemb = tf.get_variable('Weights', initializer=wemb_init)\n",
    "\n",
    "# Declare a placeholder to inject the word indeces to be projected (one-hot vectors)\n",
    "word_in = tf.placeholder(tf.int32)\n",
    "\n",
    "# Add a lookup operator to obtain the embedding of an index\n",
    "word_emb = tf.nn.embedding_lookup(Wemb, word_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Define the cosine similarity projection of an input embedding to get the nearest embeddings to the one we inject through `nearby_word` input placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Â Normalize the embedding weights to be norm 1 for the neighbour computation (hyper sphere surface radius 1)\n",
    "nemb = tf.nn.l2_normalize(Wemb, 1)\n",
    "\n",
    "# Add the nearby computation ops to check, out of a nearby_word, which are its neighbors\n",
    "nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "nearby_emb = tf.gather(nemb, nearby_word)\n",
    "\n",
    "# TODO: define the cosine similarity operation \n",
    "nearby_dist = tf.matmul(nearby_emb, nemb, transpose_b=True)\n",
    "nearby_val, nearby_idx = tf.nn.top_k(nearby_dist,\n",
    "                                     min(1000, VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word dolphin code 16350\n",
      "word dog code 2927\n",
      "word house code 167\n",
      "word barcelona code 3652\n",
      "word great code 354\n",
      "ids shape:  (5,)\n",
      "\n",
      "dolphin\n",
      "=====================================\n",
      "dolphin              1.0000\n",
      "whale                0.7471\n",
      "shark                0.7335\n",
      "turtle               0.6353\n",
      "bottlenose           0.6287\n",
      "elephant             0.6223\n",
      "whales               0.5994\n",
      "monkey               0.5933\n",
      "sharks               0.5845\n",
      "humpback             0.5782\n",
      "panda                0.5778\n",
      "turtles              0.5732\n",
      "tuna                 0.5728\n",
      "dolphins             0.5666\n",
      "crocodile            0.5658\n",
      "alligator            0.5631\n",
      "orca                 0.5618\n",
      "elephants            0.5595\n",
      "cat                  0.5568\n",
      "aquarium             0.5567\n",
      "\n",
      "dog\n",
      "=====================================\n",
      "dog                  1.0000\n",
      "cat                  0.8798\n",
      "dogs                 0.8344\n",
      "pet                  0.7450\n",
      "puppy                0.7236\n",
      "horse                0.7110\n",
      "animal               0.6817\n",
      "pig                  0.6554\n",
      "boy                  0.6545\n",
      "cats                 0.6472\n",
      "rabbit               0.6469\n",
      "goat                 0.6299\n",
      "sled                 0.6258\n",
      "monkey               0.6221\n",
      "cow                  0.6219\n",
      "rat                  0.6167\n",
      "breed                0.6090\n",
      "mad                  0.6017\n",
      "hound                0.5999\n",
      "bird                 0.5997\n",
      "\n",
      "house\n",
      "=====================================\n",
      "house                1.0000\n",
      "office               0.7582\n",
      "senate               0.7205\n",
      "room                 0.7150\n",
      "houses               0.6888\n",
      "capitol              0.6852\n",
      "building             0.6847\n",
      "home                 0.6720\n",
      "clinton              0.6707\n",
      "congressional        0.6693\n",
      "mansion              0.6651\n",
      "congress             0.6636\n",
      "floor                0.6604\n",
      "bush                 0.6594\n",
      "door                 0.6543\n",
      "once                 0.6511\n",
      "republican           0.6450\n",
      "white                0.6438\n",
      "the                  0.6434\n",
      "where                0.6394\n",
      "\n",
      "barcelona\n",
      "=====================================\n",
      "barcelona            1.0000\n",
      "madrid               0.8428\n",
      "valencia             0.8055\n",
      "sevilla              0.7542\n",
      "marseille            0.7344\n",
      "porto                0.7085\n",
      "liverpool            0.7014\n",
      "fc                   0.6964\n",
      "bayern               0.6964\n",
      "zaragoza             0.6956\n",
      "atletico             0.6923\n",
      "villarreal           0.6903\n",
      "lyon                 0.6873\n",
      "milan                0.6835\n",
      "barca                0.6823\n",
      "bilbao               0.6821\n",
      "deportivo            0.6788\n",
      "manchester           0.6733\n",
      "monaco               0.6700\n",
      "eindhoven            0.6695\n",
      "\n",
      "great\n",
      "=====================================\n",
      "great                1.0000\n",
      "greatest             0.7883\n",
      "good                 0.7593\n",
      "little               0.7586\n",
      "much                 0.7477\n",
      "well                 0.7401\n",
      "big                  0.7319\n",
      "kind                 0.7309\n",
      "important            0.7287\n",
      "there                0.7226\n",
      "way                  0.7116\n",
      "lot                  0.7113\n",
      "tremendous           0.7086\n",
      "this                 0.7080\n",
      "very                 0.7076\n",
      "perhaps              0.7075\n",
      "famous               0.7072\n",
      "brought              0.7061\n",
      "time                 0.7025\n",
      "always               0.7016\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "\n",
    "# TODO: play around with word_examples to be projected\n",
    "word_examples = ['dolphin', 'dog', 'house', 'barcelona', 'great']\n",
    "\n",
    "# make nearby function to obtain nearby words given the list of words\n",
    "# ref: https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n",
    "def nearby(sess, ids, num=20):\n",
    "    \"\"\"Prints out nearby words given a list of words.\"\"\"\n",
    "    #ids = np.array([word2idx[word] for word in words])\n",
    "    print('ids shape: ', ids.shape)\n",
    "    vals, idx = sess.run([nearby_val, nearby_idx], {nearby_word:ids})\n",
    "    for i, word_idx in enumerate(ids):\n",
    "        print(\"\\n%s\\n=====================================\" % (idx2word[word_idx]))\n",
    "        for (neighbor, distance) in zip(idx[i, :num], vals[i, :num]):\n",
    "            print(\"%-20s %6.4f\" % (idx2word[neighbor], distance))\n",
    "\n",
    "# Encode the words to their indices and infer the mapped word\n",
    "word_codes = []\n",
    "for word in word_examples:\n",
    "    try:\n",
    "        word_idx = word2idx[word]\n",
    "        print('word {} code {}'.format(word, word_idx))\n",
    "    except KeyError:\n",
    "        # if the word is not in the vocab, map to UNK (0)\n",
    "        print('WARNING: {} not in vocabulary'.format(word))\n",
    "        continue\n",
    "    word_codes.append(word_idx)\n",
    "    #print('word {} emb {}'.format(word, sess.run(word_emb, {word_in:word_idx})))\n",
    "\n",
    "# do the neighbor mapping\n",
    "nearby(sess, np.array(word_codes, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
