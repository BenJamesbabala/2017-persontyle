{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying with artificial neurons\n",
    "\n",
    "## Introduction: The perceptron\n",
    "\n",
    "The first example of this notebook is a binary classifier by means of the Logistic Regression operation. This model is also called Perceptron or Artificial Neuron, depicted in the figure below.\n",
    "\n",
    "![perceptron](assets/perceptron.png)\n",
    "\n",
    "There we have a set of inputs {x1, x2, x3}, a set of weights {w1, w2, w3}, a bias factor {b} and an activation function {f}, and the following operations are applied:\n",
    "\n",
    "$$y = f(\\sum_{m=1}^{M} x_i * w_i + b)$$\n",
    "\n",
    "To actually make it a binary classifier we must place a specific type of activation function called Sigmoid: \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{(1 + {e}^{-z})}$$ where $$z = x * w + b$$\n",
    "\n",
    "The Sigmoid shape is depicted in the figure below:\n",
    "\n",
    "![sigmoid](assets/sigmoid.png)\n",
    "\n",
    "We can see how, depending on the weights and biases (in the depicted figure all sigmoids have scalar values, so only one input x1 would be injected) there is a ridge bounding the outputs between (0, 1), which can be interpreted as a probability output depending on the inputs that activate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1: It is a number or is it noise?\n",
    "\n",
    "In this example we will classify whether an image is noise or a MNIST digit:\n",
    "* MNIST dataset contains images of 10 handwritten digit classes {0, 1, 2, 3, 4, ..., 9}. Each class contains 6.000 images of 28x28 pixels.\n",
    "\n",
    "We will use 50.000 images for training and 10.000 for testing our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import print utility and timer\n",
    "from __future__ import print_function\n",
    "import timeit\n",
    "# First import tensorflow and the data reader\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "# numpy for matrix utilities\n",
    "import numpy as np\n",
    "from utils import plot_samples\n",
    "# import plot utilities\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Computed unrolled size:  784\n"
     ]
    }
   ],
   "source": [
    "# define IMG dimensions\n",
    "IMG_SIZE=28\n",
    "# Import mnist data\n",
    "mnist = input_data.read_data_sets('data/mnist/', one_hot=True)\n",
    "# Make the random images generator (28x28 withdrawn from a random uniform distribution)\n",
    "def make_random_batch(batch_size):\n",
    "    # Generate batch_size images of size image_size x image_size\n",
    "    rimg = np.random.uniform(low=0., high=1., size=(batch_size, IMG_SIZE * IMG_SIZE))\n",
    "    return rimg\n",
    "# Define num of pixels that will be input to models\n",
    "unrolled_size = IMG_SIZE * IMG_SIZE\n",
    "print(\"Computed unrolled size: \", unrolled_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e6ece5e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize some MNIST samples\n",
    "batch_x_real, _ = mnist.train.next_batch(100)\n",
    "batch_x_real = batch_x_real.reshape((100, 28, 28))\n",
    "plot_samples(batch_x_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Define the model operation of logistic regression equation and the placeholder to insert groundtruth binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare the model, recall logistic regression equation\n",
    "# Define weights matrix (from unrolled_size inputs to 1 classification output (noise(0)/mnist(1)))\n",
    "W = tf.Variable(tf.zeros([unrolled_size, 1]))\n",
    "# the bias is summing just a scalar output, so dimension 1\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "# define an input placeholder to inject the vectorized images\n",
    "# None indicates we don't know batch_size yet, will be specified when running the training\n",
    "x = tf.placeholder(tf.float32, [None, unrolled_size]) \n",
    "# TODO: Logistic Regression equation implementation\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# apply sigmoid to get final predictions\n",
    "out = tf.sigmoid(y)\n",
    "\n",
    "# TODO: Now we define the placeholder to place the flag (0 or 1) as output examples\n",
    "y_ = tf.pla\n",
    "\n",
    "# Now call the sigmoid cross entropy with logits to compute the loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "# define the gradients update operation with learning rate of 0.05\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize the TensorFlow session to run the operations Graph \"on the fly\" (not usual in production code)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# specify number of epochs to run through whole dataset\n",
    "num_epochs = 1 # approx 55 s / epoch on laptop (macbook pro 13\" i7 end 2011 w/ 8GB RAM) w/ batch_size = 10\n",
    "# compute total amount of batches to be run\n",
    "train_size = 50000\n",
    "num_batches = int(train_size * num_epochs)\n",
    "# specify batch_size \n",
    "batch_size = 10\n",
    "# print loss after this amount of batches\n",
    "print_every = 10000\n",
    "tr_losses = []\n",
    "\n",
    "print('Training...')\n",
    "beg_t = timeit.default_timer()\n",
    "# Run the training iterations\n",
    "for curr_batch in range(num_batches):\n",
    "    # get the batch of training images (to be injected to x placeholder)\n",
    "    batch_x_real, _ = mnist.train.next_batch(batch_size)\n",
    "    # create the batch of labels (to be injected to y_ placeholder)\n",
    "    batch_y_real = np.ones((batch_size, 1))\n",
    "    # generate the batch of random images (to be injected to x placeholder)\n",
    "    batch_x_random = make_random_batch(batch_size)\n",
    "    # create the batch of 0 labels (to be injectd to y_ placeholder)\n",
    "    batch_y_random = np.zeros((batch_size, 1))\n",
    "    # merge both batches into one and run update\n",
    "    batch_x = np.concatenate((batch_x_real, batch_x_random), axis=0)\n",
    "    batch_y = np.concatenate((batch_y_real, batch_y_random), axis=0)\n",
    "    # run model update (learning stage over a batch of samples)\n",
    "    tr_loss , _= sess.run([loss, train_step], feed_dict={x: batch_x, y_:batch_y})\n",
    "    tr_losses.append(tr_loss)\n",
    "    if (curr_batch + 1) % print_every == 0:\n",
    "        print('Batch {}/{} training loss: {:.6f}'.format(curr_batch + 1, num_batches, tr_loss))\n",
    "end_t = timeit.default_timer()\n",
    "print('Total time training {} epochs: {} s'.format(num_epochs, end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.semilogy(tr_losses)\n",
    "plt.xlabel('Batch index')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Call the right test op to evaluate the performance of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test trained model\n",
    "# generate the random test images\n",
    "te_x_random = make_random_batch(10000)\n",
    "te_y_random = np.zeros((10000, 1))\n",
    "# cache the real test images\n",
    "te_x_real = mnist.test.images\n",
    "te_y_real = np.ones((10000, 1))\n",
    "# total test batches\n",
    "te_x_batch = np.concatenate((te_x_random, te_x_real), axis=0)\n",
    "te_y_batch = np.concatenate((te_y_random, te_y_real), axis=0)\n",
    "# define the accuracy computation op\n",
    "# NOTE: the sigmoid output is rounded so that if out >= 0.5 --> predicts 1, otherwise predicts 0\n",
    "correct_prediction = tf.equal(tf.round(out), y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# TODO: print Accuracy computation running the accuracy op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2: What number is it? Scaling up to multiple classes\n",
    "Now that a binary classification task has been solved we will see its natural extension: a multiclass classifier.\n",
    "This can be done by means of a softmax layer. The softmax layer is a parallel arrangement of sigmoidal neurons (*Output Layer* in the image below), where every neuron indicates the amount of probability that the input features (*Input Layer* in the image below) belong to a certain class.\n",
    "\n",
    "![softmax](assets/softmax_img.png)\n",
    "\n",
    "\n",
    "As it is a probability distribution between the possible classes, all of them must sum up to 1. So the softmax formulation is the following one:\n",
    "\n",
    "$$y = \\frac{e^{x^T * w_k}}{\\sum_{n=1}^{N} e^{x^T * w_n}}$$\n",
    "\n",
    "where x and w are vectors representing inputs $x$ and k-th layer weights $w_k$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reset graph here\n",
    "tf.reset_default_graph()\n",
    "# initialize the TensorFlow session to run the operations Graph \"on the fly\" (not usual in production code)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First, define the new weights and biases to express the multiple output neurons\n",
    "# TODO: Define weights matrix (from unrolled_size inputs to 10 classification outputs (10 MNIST digits)\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# TODO: define the biases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# TODO: define an input placeholder to inject the vectorized images\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# TODO: equation implementation\n",
    "y = tf.matmul(x, W) + b\n",
    "# apply sigmoid to get final predictions\n",
    "out = tf.nn.softmax(y)\n",
    "\n",
    "# TODO: Now we define the placeholder to place the classes\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# TODO: Now call the softmax cross entropy with logits to compute the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "# TODO: define the gradients update operation with learning rate of 0.05\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Batch 10/10000 training loss: 1.904934\n",
      "Batch 20/10000 training loss: 1.577403\n",
      "Batch 30/10000 training loss: 1.331500\n",
      "Batch 40/10000 training loss: 1.256337\n",
      "Batch 50/10000 training loss: 1.059200\n",
      "Batch 60/10000 training loss: 1.091902\n",
      "Batch 70/10000 training loss: 0.862689\n",
      "Batch 80/10000 training loss: 0.943952\n",
      "Batch 90/10000 training loss: 0.893621\n",
      "Batch 100/10000 training loss: 0.656957\n",
      "Batch 110/10000 training loss: 0.698352\n",
      "Batch 120/10000 training loss: 0.759005\n",
      "Batch 130/10000 training loss: 0.680438\n",
      "Batch 140/10000 training loss: 0.745860\n",
      "Batch 150/10000 training loss: 0.609099\n",
      "Batch 160/10000 training loss: 0.734338\n",
      "Batch 170/10000 training loss: 0.622142\n",
      "Batch 180/10000 training loss: 0.569785\n",
      "Batch 190/10000 training loss: 0.650567\n",
      "Batch 200/10000 training loss: 0.596481\n",
      "Batch 210/10000 training loss: 0.505776\n",
      "Batch 220/10000 training loss: 0.552786\n",
      "Batch 230/10000 training loss: 0.571930\n",
      "Batch 240/10000 training loss: 0.613958\n",
      "Batch 250/10000 training loss: 0.591480\n",
      "Batch 260/10000 training loss: 0.619010\n",
      "Batch 270/10000 training loss: 0.508913\n",
      "Batch 280/10000 training loss: 0.654946\n",
      "Batch 290/10000 training loss: 0.658551\n",
      "Batch 300/10000 training loss: 0.581790\n",
      "Batch 310/10000 training loss: 0.574817\n",
      "Batch 320/10000 training loss: 0.489678\n",
      "Batch 330/10000 training loss: 0.513514\n",
      "Batch 340/10000 training loss: 0.454835\n",
      "Batch 350/10000 training loss: 0.510024\n",
      "Batch 360/10000 training loss: 0.499374\n",
      "Batch 370/10000 training loss: 0.472207\n",
      "Batch 380/10000 training loss: 0.511824\n",
      "Batch 390/10000 training loss: 0.452357\n",
      "Batch 400/10000 training loss: 0.515733\n",
      "Batch 410/10000 training loss: 0.466348\n",
      "Batch 420/10000 training loss: 0.435861\n",
      "Batch 430/10000 training loss: 0.436166\n",
      "Batch 440/10000 training loss: 0.663872\n",
      "Batch 450/10000 training loss: 0.430926\n",
      "Batch 460/10000 training loss: 0.521073\n",
      "Batch 470/10000 training loss: 0.551613\n",
      "Batch 480/10000 training loss: 0.484117\n",
      "Batch 490/10000 training loss: 0.449469\n",
      "Batch 500/10000 training loss: 0.458384\n",
      "Batch 510/10000 training loss: 0.434720\n",
      "Batch 520/10000 training loss: 0.543191\n",
      "Batch 530/10000 training loss: 0.536509\n",
      "Batch 540/10000 training loss: 0.518156\n",
      "Batch 550/10000 training loss: 0.349464\n",
      "Batch 560/10000 training loss: 0.430387\n",
      "Batch 570/10000 training loss: 0.395209\n",
      "Batch 580/10000 training loss: 0.432813\n",
      "Batch 590/10000 training loss: 0.637754\n",
      "Batch 600/10000 training loss: 0.355711\n",
      "Batch 610/10000 training loss: 0.386219\n",
      "Batch 620/10000 training loss: 0.395037\n",
      "Batch 630/10000 training loss: 0.333309\n",
      "Batch 640/10000 training loss: 0.458928\n",
      "Batch 650/10000 training loss: 0.338574\n",
      "Batch 660/10000 training loss: 0.287319\n",
      "Batch 670/10000 training loss: 0.555862\n",
      "Batch 680/10000 training loss: 0.516379\n",
      "Batch 690/10000 training loss: 0.419611\n",
      "Batch 700/10000 training loss: 0.461337\n",
      "Batch 710/10000 training loss: 0.338401\n",
      "Batch 720/10000 training loss: 0.291774\n",
      "Batch 730/10000 training loss: 0.308261\n",
      "Batch 740/10000 training loss: 0.501744\n",
      "Batch 750/10000 training loss: 0.308962\n",
      "Batch 760/10000 training loss: 0.353267\n",
      "Batch 770/10000 training loss: 0.319510\n",
      "Batch 780/10000 training loss: 0.529679\n",
      "Batch 790/10000 training loss: 0.440266\n",
      "Batch 800/10000 training loss: 0.470252\n",
      "Batch 810/10000 training loss: 0.427869\n",
      "Batch 820/10000 training loss: 0.505772\n",
      "Batch 830/10000 training loss: 0.435820\n",
      "Batch 840/10000 training loss: 0.421079\n",
      "Batch 850/10000 training loss: 0.491988\n",
      "Batch 860/10000 training loss: 0.406654\n",
      "Batch 870/10000 training loss: 0.464972\n",
      "Batch 880/10000 training loss: 0.574952\n",
      "Batch 890/10000 training loss: 0.327487\n",
      "Batch 900/10000 training loss: 0.373470\n",
      "Batch 910/10000 training loss: 0.322514\n",
      "Batch 920/10000 training loss: 0.431770\n",
      "Batch 930/10000 training loss: 0.453292\n",
      "Batch 940/10000 training loss: 0.284654\n",
      "Batch 950/10000 training loss: 0.582904\n",
      "Batch 960/10000 training loss: 0.484881\n",
      "Batch 970/10000 training loss: 0.353044\n",
      "Batch 980/10000 training loss: 0.383430\n",
      "Batch 990/10000 training loss: 0.306037\n",
      "Batch 1000/10000 training loss: 0.379164\n",
      "Batch 1010/10000 training loss: 0.234245\n",
      "Batch 1020/10000 training loss: 0.377568\n",
      "Batch 1030/10000 training loss: 0.366312\n",
      "Batch 1040/10000 training loss: 0.375792\n",
      "Batch 1050/10000 training loss: 0.380891\n",
      "Batch 1060/10000 training loss: 0.443263\n",
      "Batch 1070/10000 training loss: 0.360772\n",
      "Batch 1080/10000 training loss: 0.612336\n",
      "Batch 1090/10000 training loss: 0.247311\n",
      "Batch 1100/10000 training loss: 0.312389\n",
      "Batch 1110/10000 training loss: 0.346521\n",
      "Batch 1120/10000 training loss: 0.509217\n",
      "Batch 1130/10000 training loss: 0.445379\n",
      "Batch 1140/10000 training loss: 0.328047\n",
      "Batch 1150/10000 training loss: 0.393111\n",
      "Batch 1160/10000 training loss: 0.438121\n",
      "Batch 1170/10000 training loss: 0.339661\n",
      "Batch 1180/10000 training loss: 0.431122\n",
      "Batch 1190/10000 training loss: 0.416482\n",
      "Batch 1200/10000 training loss: 0.374746\n",
      "Batch 1210/10000 training loss: 0.349000\n",
      "Batch 1220/10000 training loss: 0.275750\n",
      "Batch 1230/10000 training loss: 0.330012\n",
      "Batch 1240/10000 training loss: 0.261323\n",
      "Batch 1250/10000 training loss: 0.358689\n",
      "Batch 1260/10000 training loss: 0.441335\n",
      "Batch 1270/10000 training loss: 0.386391\n",
      "Batch 1280/10000 training loss: 0.390505\n",
      "Batch 1290/10000 training loss: 0.470336\n",
      "Batch 1300/10000 training loss: 0.508844\n",
      "Batch 1310/10000 training loss: 0.398559\n",
      "Batch 1320/10000 training loss: 0.356984\n",
      "Batch 1330/10000 training loss: 0.348436\n",
      "Batch 1340/10000 training loss: 0.312703\n",
      "Batch 1350/10000 training loss: 0.314209\n",
      "Batch 1360/10000 training loss: 0.329813\n",
      "Batch 1370/10000 training loss: 0.505163\n",
      "Batch 1380/10000 training loss: 0.383027\n",
      "Batch 1390/10000 training loss: 0.360990\n",
      "Batch 1400/10000 training loss: 0.344856\n",
      "Batch 1410/10000 training loss: 0.342652\n",
      "Batch 1420/10000 training loss: 0.404737\n",
      "Batch 1430/10000 training loss: 0.334843\n",
      "Batch 1440/10000 training loss: 0.339233\n",
      "Batch 1450/10000 training loss: 0.321054\n",
      "Batch 1460/10000 training loss: 0.375715\n",
      "Batch 1470/10000 training loss: 0.457941\n",
      "Batch 1480/10000 training loss: 0.281849\n",
      "Batch 1490/10000 training loss: 0.424534\n",
      "Batch 1500/10000 training loss: 0.320342\n",
      "Batch 1510/10000 training loss: 0.407938\n",
      "Batch 1520/10000 training loss: 0.363256\n",
      "Batch 1530/10000 training loss: 0.512386\n",
      "Batch 1540/10000 training loss: 0.290057\n",
      "Batch 1550/10000 training loss: 0.367136\n",
      "Batch 1560/10000 training loss: 0.271361\n",
      "Batch 1570/10000 training loss: 0.340490\n",
      "Batch 1580/10000 training loss: 0.350322\n",
      "Batch 1590/10000 training loss: 0.289911\n",
      "Batch 1600/10000 training loss: 0.405815\n",
      "Batch 1610/10000 training loss: 0.358141\n",
      "Batch 1620/10000 training loss: 0.561011\n",
      "Batch 1630/10000 training loss: 0.367336\n",
      "Batch 1640/10000 training loss: 0.255441\n",
      "Batch 1650/10000 training loss: 0.193074\n",
      "Batch 1660/10000 training loss: 0.350798\n",
      "Batch 1670/10000 training loss: 0.328009\n",
      "Batch 1680/10000 training loss: 0.362465\n",
      "Batch 1690/10000 training loss: 0.253799\n",
      "Batch 1700/10000 training loss: 0.247197\n",
      "Batch 1710/10000 training loss: 0.322550\n",
      "Batch 1720/10000 training loss: 0.375340\n",
      "Batch 1730/10000 training loss: 0.428268\n",
      "Batch 1740/10000 training loss: 0.375217\n",
      "Batch 1750/10000 training loss: 0.363256\n",
      "Batch 1760/10000 training loss: 0.479021\n",
      "Batch 1770/10000 training loss: 0.318267\n",
      "Batch 1780/10000 training loss: 0.319494\n",
      "Batch 1790/10000 training loss: 0.385632\n",
      "Batch 1800/10000 training loss: 0.246721\n",
      "Batch 1810/10000 training loss: 0.214884\n",
      "Batch 1820/10000 training loss: 0.411946\n",
      "Batch 1830/10000 training loss: 0.263580\n",
      "Batch 1840/10000 training loss: 0.399354\n",
      "Batch 1850/10000 training loss: 0.399075\n",
      "Batch 1860/10000 training loss: 0.487399\n",
      "Batch 1870/10000 training loss: 0.327189\n",
      "Batch 1880/10000 training loss: 0.325416\n",
      "Batch 1890/10000 training loss: 0.265319\n",
      "Batch 1900/10000 training loss: 0.282209\n",
      "Batch 1910/10000 training loss: 0.391935\n",
      "Batch 1920/10000 training loss: 0.343313\n",
      "Batch 1930/10000 training loss: 0.251312\n",
      "Batch 1940/10000 training loss: 0.395152\n",
      "Batch 1950/10000 training loss: 0.296346\n",
      "Batch 1960/10000 training loss: 0.455669\n",
      "Batch 1970/10000 training loss: 0.339941\n",
      "Batch 1980/10000 training loss: 0.270903\n",
      "Batch 1990/10000 training loss: 0.324277\n",
      "Batch 2000/10000 training loss: 0.294128\n",
      "Batch 2010/10000 training loss: 0.298062\n",
      "Batch 2020/10000 training loss: 0.400514\n",
      "Batch 2030/10000 training loss: 0.277459\n",
      "Batch 2040/10000 training loss: 0.295976\n",
      "Batch 2050/10000 training loss: 0.277406\n",
      "Batch 2060/10000 training loss: 0.288370\n",
      "Batch 2070/10000 training loss: 0.440035\n",
      "Batch 2080/10000 training loss: 0.363842\n",
      "Batch 2090/10000 training loss: 0.520493\n",
      "Batch 2100/10000 training loss: 0.385275\n",
      "Batch 2110/10000 training loss: 0.380192\n",
      "Batch 2120/10000 training loss: 0.240608\n",
      "Batch 2130/10000 training loss: 0.282835\n",
      "Batch 2140/10000 training loss: 0.375722\n",
      "Batch 2150/10000 training loss: 0.416171\n",
      "Batch 2160/10000 training loss: 0.251489\n",
      "Batch 2170/10000 training loss: 0.206926\n",
      "Batch 2180/10000 training loss: 0.254168\n",
      "Batch 2190/10000 training loss: 0.521796\n",
      "Batch 2200/10000 training loss: 0.283579\n",
      "Batch 2210/10000 training loss: 0.372477\n",
      "Batch 2220/10000 training loss: 0.343800\n",
      "Batch 2230/10000 training loss: 0.334474\n",
      "Batch 2240/10000 training loss: 0.289947\n",
      "Batch 2250/10000 training loss: 0.224366\n",
      "Batch 2260/10000 training loss: 0.338363\n",
      "Batch 2270/10000 training loss: 0.229904\n",
      "Batch 2280/10000 training loss: 0.382493\n",
      "Batch 2290/10000 training loss: 0.266730\n",
      "Batch 2300/10000 training loss: 0.390634\n",
      "Batch 2310/10000 training loss: 0.265915\n",
      "Batch 2320/10000 training loss: 0.359757\n",
      "Batch 2330/10000 training loss: 0.259892\n",
      "Batch 2340/10000 training loss: 0.361354\n",
      "Batch 2350/10000 training loss: 0.238384\n",
      "Batch 2360/10000 training loss: 0.345318\n",
      "Batch 2370/10000 training loss: 0.294468\n",
      "Batch 2380/10000 training loss: 0.399161\n",
      "Batch 2390/10000 training loss: 0.271892\n",
      "Batch 2400/10000 training loss: 0.351849\n",
      "Batch 2410/10000 training loss: 0.403159\n",
      "Batch 2420/10000 training loss: 0.311676\n",
      "Batch 2430/10000 training loss: 0.260422\n",
      "Batch 2440/10000 training loss: 0.279202\n",
      "Batch 2450/10000 training loss: 0.360061\n",
      "Batch 2460/10000 training loss: 0.250579\n",
      "Batch 2470/10000 training loss: 0.375154\n",
      "Batch 2480/10000 training loss: 0.332273\n",
      "Batch 2490/10000 training loss: 0.354681\n",
      "Batch 2500/10000 training loss: 0.365978\n",
      "Batch 2510/10000 training loss: 0.269029\n",
      "Batch 2520/10000 training loss: 0.346316\n",
      "Batch 2530/10000 training loss: 0.305720\n",
      "Batch 2540/10000 training loss: 0.212210\n",
      "Batch 2550/10000 training loss: 0.302128\n",
      "Batch 2560/10000 training loss: 0.367321\n",
      "Batch 2570/10000 training loss: 0.399175\n",
      "Batch 2580/10000 training loss: 0.402110\n",
      "Batch 2590/10000 training loss: 0.345372\n",
      "Batch 2600/10000 training loss: 0.360400\n",
      "Batch 2610/10000 training loss: 0.197373\n",
      "Batch 2620/10000 training loss: 0.374153\n",
      "Batch 2630/10000 training loss: 0.651216\n",
      "Batch 2640/10000 training loss: 0.433291\n",
      "Batch 2650/10000 training loss: 0.349152\n",
      "Batch 2660/10000 training loss: 0.432502\n",
      "Batch 2670/10000 training loss: 0.206891\n",
      "Batch 2680/10000 training loss: 0.258648\n",
      "Batch 2690/10000 training loss: 0.342567\n",
      "Batch 2700/10000 training loss: 0.340919\n",
      "Batch 2710/10000 training loss: 0.366439\n",
      "Batch 2720/10000 training loss: 0.381305\n",
      "Batch 2730/10000 training loss: 0.273548\n",
      "Batch 2740/10000 training loss: 0.385427\n",
      "Batch 2750/10000 training loss: 0.351766\n",
      "Batch 2760/10000 training loss: 0.271512\n",
      "Batch 2770/10000 training loss: 0.283507\n",
      "Batch 2780/10000 training loss: 0.422165\n",
      "Batch 2790/10000 training loss: 0.395302\n",
      "Batch 2800/10000 training loss: 0.268511\n",
      "Batch 2810/10000 training loss: 0.258956\n",
      "Batch 2820/10000 training loss: 0.332835\n",
      "Batch 2830/10000 training loss: 0.289249\n",
      "Batch 2840/10000 training loss: 0.382353\n",
      "Batch 2850/10000 training loss: 0.228734\n",
      "Batch 2860/10000 training loss: 0.423535\n",
      "Batch 2870/10000 training loss: 0.383916\n",
      "Batch 2880/10000 training loss: 0.290008\n",
      "Batch 2890/10000 training loss: 0.445757\n",
      "Batch 2900/10000 training loss: 0.256249\n",
      "Batch 2910/10000 training loss: 0.562810\n",
      "Batch 2920/10000 training loss: 0.345975\n",
      "Batch 2930/10000 training loss: 0.320151\n",
      "Batch 2940/10000 training loss: 0.237297\n",
      "Batch 2950/10000 training loss: 0.313118\n",
      "Batch 2960/10000 training loss: 0.340251\n",
      "Batch 2970/10000 training loss: 0.439052\n",
      "Batch 2980/10000 training loss: 0.274834\n",
      "Batch 2990/10000 training loss: 0.226152\n",
      "Batch 3000/10000 training loss: 0.214200\n",
      "Batch 3010/10000 training loss: 0.340768\n",
      "Batch 3020/10000 training loss: 0.300518\n",
      "Batch 3030/10000 training loss: 0.418073\n",
      "Batch 3040/10000 training loss: 0.307901\n",
      "Batch 3050/10000 training loss: 0.327982\n",
      "Batch 3060/10000 training loss: 0.460381\n",
      "Batch 3070/10000 training loss: 0.261543\n",
      "Batch 3080/10000 training loss: 0.350319\n",
      "Batch 3090/10000 training loss: 0.290717\n",
      "Batch 3100/10000 training loss: 0.279047\n",
      "Batch 3110/10000 training loss: 0.325249\n",
      "Batch 3120/10000 training loss: 0.576522\n",
      "Batch 3130/10000 training loss: 0.357914\n",
      "Batch 3140/10000 training loss: 0.405483\n",
      "Batch 3150/10000 training loss: 0.430384\n",
      "Batch 3160/10000 training loss: 0.298140\n",
      "Batch 3170/10000 training loss: 0.392959\n",
      "Batch 3180/10000 training loss: 0.307019\n",
      "Batch 3190/10000 training loss: 0.256832\n",
      "Batch 3200/10000 training loss: 0.358633\n",
      "Batch 3210/10000 training loss: 0.260286\n",
      "Batch 3220/10000 training loss: 0.302983\n",
      "Batch 3230/10000 training loss: 0.317104\n",
      "Batch 3240/10000 training loss: 0.286862\n",
      "Batch 3250/10000 training loss: 0.322277\n",
      "Batch 3260/10000 training loss: 0.371485\n",
      "Batch 3270/10000 training loss: 0.394766\n",
      "Batch 3280/10000 training loss: 0.405337\n",
      "Batch 3290/10000 training loss: 0.364370\n",
      "Batch 3300/10000 training loss: 0.241388\n",
      "Batch 3310/10000 training loss: 0.268070\n",
      "Batch 3320/10000 training loss: 0.366676\n",
      "Batch 3330/10000 training loss: 0.360607\n",
      "Batch 3340/10000 training loss: 0.322743\n",
      "Batch 3350/10000 training loss: 0.288577\n",
      "Batch 3360/10000 training loss: 0.366100\n",
      "Batch 3370/10000 training loss: 0.456592\n",
      "Batch 3380/10000 training loss: 0.324190\n",
      "Batch 3390/10000 training loss: 0.420969\n",
      "Batch 3400/10000 training loss: 0.290476\n",
      "Batch 3410/10000 training loss: 0.266295\n",
      "Batch 3420/10000 training loss: 0.337323\n",
      "Batch 3430/10000 training loss: 0.286660\n",
      "Batch 3440/10000 training loss: 0.247122\n",
      "Batch 3450/10000 training loss: 0.310411\n",
      "Batch 3460/10000 training loss: 0.398844\n",
      "Batch 3470/10000 training loss: 0.216263\n",
      "Batch 3480/10000 training loss: 0.318461\n",
      "Batch 3490/10000 training loss: 0.192411\n",
      "Batch 3500/10000 training loss: 0.403342\n",
      "Batch 3510/10000 training loss: 0.266226\n",
      "Batch 3520/10000 training loss: 0.359728\n",
      "Batch 3530/10000 training loss: 0.275149\n",
      "Batch 3540/10000 training loss: 0.354211\n",
      "Batch 3550/10000 training loss: 0.398174\n",
      "Batch 3560/10000 training loss: 0.359672\n",
      "Batch 3570/10000 training loss: 0.269452\n",
      "Batch 3580/10000 training loss: 0.307788\n",
      "Batch 3590/10000 training loss: 0.321749\n",
      "Batch 3600/10000 training loss: 0.231206\n",
      "Batch 3610/10000 training loss: 0.288115\n",
      "Batch 3620/10000 training loss: 0.366716\n",
      "Batch 3630/10000 training loss: 0.330565\n",
      "Batch 3640/10000 training loss: 0.163100\n",
      "Batch 3650/10000 training loss: 0.313609\n",
      "Batch 3660/10000 training loss: 0.334717\n",
      "Batch 3670/10000 training loss: 0.301942\n",
      "Batch 3680/10000 training loss: 0.279861\n",
      "Batch 3690/10000 training loss: 0.213566\n",
      "Batch 3700/10000 training loss: 0.410657\n",
      "Batch 3710/10000 training loss: 0.255005\n",
      "Batch 3720/10000 training loss: 0.332099\n",
      "Batch 3730/10000 training loss: 0.389873\n",
      "Batch 3740/10000 training loss: 0.327288\n",
      "Batch 3750/10000 training loss: 0.261266\n",
      "Batch 3760/10000 training loss: 0.373541\n",
      "Batch 3770/10000 training loss: 0.302521\n",
      "Batch 3780/10000 training loss: 0.233671\n",
      "Batch 3790/10000 training loss: 0.386418\n",
      "Batch 3800/10000 training loss: 0.433626\n",
      "Batch 3810/10000 training loss: 0.278556\n",
      "Batch 3820/10000 training loss: 0.240726\n",
      "Batch 3830/10000 training loss: 0.235288\n",
      "Batch 3840/10000 training loss: 0.315764\n",
      "Batch 3850/10000 training loss: 0.301628\n",
      "Batch 3860/10000 training loss: 0.287667\n",
      "Batch 3870/10000 training loss: 0.299427\n",
      "Batch 3880/10000 training loss: 0.331351\n",
      "Batch 3890/10000 training loss: 0.308344\n",
      "Batch 3900/10000 training loss: 0.263999\n",
      "Batch 3910/10000 training loss: 0.237960\n",
      "Batch 3920/10000 training loss: 0.220872\n",
      "Batch 3930/10000 training loss: 0.348294\n",
      "Batch 3940/10000 training loss: 0.425868\n",
      "Batch 3950/10000 training loss: 0.284573\n",
      "Batch 3960/10000 training loss: 0.298964\n",
      "Batch 3970/10000 training loss: 0.203777\n",
      "Batch 3980/10000 training loss: 0.317884\n",
      "Batch 3990/10000 training loss: 0.303427\n",
      "Batch 4000/10000 training loss: 0.349286\n",
      "Batch 4010/10000 training loss: 0.335662\n",
      "Batch 4020/10000 training loss: 0.271829\n",
      "Batch 4030/10000 training loss: 0.317890\n",
      "Batch 4040/10000 training loss: 0.287539\n",
      "Batch 4050/10000 training loss: 0.243971\n",
      "Batch 4060/10000 training loss: 0.498243\n",
      "Batch 4070/10000 training loss: 0.290807\n",
      "Batch 4080/10000 training loss: 0.415092\n",
      "Batch 4090/10000 training loss: 0.275296\n",
      "Batch 4100/10000 training loss: 0.523071\n",
      "Batch 4110/10000 training loss: 0.387916\n",
      "Batch 4120/10000 training loss: 0.199357\n",
      "Batch 4130/10000 training loss: 0.358042\n",
      "Batch 4140/10000 training loss: 0.353116\n",
      "Batch 4150/10000 training loss: 0.394170\n",
      "Batch 4160/10000 training loss: 0.320850\n",
      "Batch 4170/10000 training loss: 0.262886\n",
      "Batch 4180/10000 training loss: 0.227664\n",
      "Batch 4190/10000 training loss: 0.375954\n",
      "Batch 4200/10000 training loss: 0.262526\n",
      "Batch 4210/10000 training loss: 0.415743\n",
      "Batch 4220/10000 training loss: 0.293721\n",
      "Batch 4230/10000 training loss: 0.336569\n",
      "Batch 4240/10000 training loss: 0.370700\n",
      "Batch 4250/10000 training loss: 0.209728\n",
      "Batch 4260/10000 training loss: 0.305307\n",
      "Batch 4270/10000 training loss: 0.355496\n",
      "Batch 4280/10000 training loss: 0.310890\n",
      "Batch 4290/10000 training loss: 0.349053\n",
      "Batch 4300/10000 training loss: 0.445971\n",
      "Batch 4310/10000 training loss: 0.390658\n",
      "Batch 4320/10000 training loss: 0.385365\n",
      "Batch 4330/10000 training loss: 0.220948\n",
      "Batch 4340/10000 training loss: 0.339598\n",
      "Batch 4350/10000 training loss: 0.346043\n",
      "Batch 4360/10000 training loss: 0.384420\n",
      "Batch 4370/10000 training loss: 0.299127\n",
      "Batch 4380/10000 training loss: 0.303169\n",
      "Batch 4390/10000 training loss: 0.261939\n",
      "Batch 4400/10000 training loss: 0.356680\n",
      "Batch 4410/10000 training loss: 0.348576\n",
      "Batch 4420/10000 training loss: 0.190004\n",
      "Batch 4430/10000 training loss: 0.247769\n",
      "Batch 4440/10000 training loss: 0.311646\n",
      "Batch 4450/10000 training loss: 0.258002\n",
      "Batch 4460/10000 training loss: 0.274849\n",
      "Batch 4470/10000 training loss: 0.244852\n",
      "Batch 4480/10000 training loss: 0.377606\n",
      "Batch 4490/10000 training loss: 0.321787\n",
      "Batch 4500/10000 training loss: 0.314614\n",
      "Batch 4510/10000 training loss: 0.287192\n",
      "Batch 4520/10000 training loss: 0.413070\n",
      "Batch 4530/10000 training loss: 0.234649\n",
      "Batch 4540/10000 training loss: 0.406163\n",
      "Batch 4550/10000 training loss: 0.246954\n",
      "Batch 4560/10000 training loss: 0.255119\n",
      "Batch 4570/10000 training loss: 0.155953\n",
      "Batch 4580/10000 training loss: 0.229039\n",
      "Batch 4590/10000 training loss: 0.354481\n",
      "Batch 4600/10000 training loss: 0.361654\n",
      "Batch 4610/10000 training loss: 0.254683\n",
      "Batch 4620/10000 training loss: 0.298461\n",
      "Batch 4630/10000 training loss: 0.258408\n",
      "Batch 4640/10000 training loss: 0.167175\n",
      "Batch 4650/10000 training loss: 0.196318\n",
      "Batch 4660/10000 training loss: 0.245526\n",
      "Batch 4670/10000 training loss: 0.241880\n",
      "Batch 4680/10000 training loss: 0.253127\n",
      "Batch 4690/10000 training loss: 0.229412\n",
      "Batch 4700/10000 training loss: 0.252788\n",
      "Batch 4710/10000 training loss: 0.216516\n",
      "Batch 4720/10000 training loss: 0.383340\n",
      "Batch 4730/10000 training loss: 0.374700\n",
      "Batch 4740/10000 training loss: 0.275320\n",
      "Batch 4750/10000 training loss: 0.261314\n",
      "Batch 4760/10000 training loss: 0.336962\n",
      "Batch 4770/10000 training loss: 0.362407\n",
      "Batch 4780/10000 training loss: 0.304173\n",
      "Batch 4790/10000 training loss: 0.319127\n",
      "Batch 4800/10000 training loss: 0.298265\n",
      "Batch 4810/10000 training loss: 0.201897\n",
      "Batch 4820/10000 training loss: 0.163083\n",
      "Batch 4830/10000 training loss: 0.344565\n",
      "Batch 4840/10000 training loss: 0.176778\n",
      "Batch 4850/10000 training loss: 0.331643\n",
      "Batch 4860/10000 training loss: 0.319547\n",
      "Batch 4870/10000 training loss: 0.367843\n",
      "Batch 4880/10000 training loss: 0.340973\n",
      "Batch 4890/10000 training loss: 0.255786\n",
      "Batch 4900/10000 training loss: 0.490838\n",
      "Batch 4910/10000 training loss: 0.298544\n",
      "Batch 4920/10000 training loss: 0.201150\n",
      "Batch 4930/10000 training loss: 0.264461\n",
      "Batch 4940/10000 training loss: 0.384302\n",
      "Batch 4950/10000 training loss: 0.443678\n",
      "Batch 4960/10000 training loss: 0.299323\n",
      "Batch 4970/10000 training loss: 0.325948\n",
      "Batch 4980/10000 training loss: 0.369462\n",
      "Batch 4990/10000 training loss: 0.186740\n",
      "Batch 5000/10000 training loss: 0.321022\n",
      "Batch 5010/10000 training loss: 0.315830\n",
      "Batch 5020/10000 training loss: 0.372941\n",
      "Batch 5030/10000 training loss: 0.351262\n",
      "Batch 5040/10000 training loss: 0.289431\n",
      "Batch 5050/10000 training loss: 0.277323\n",
      "Batch 5060/10000 training loss: 0.307382\n",
      "Batch 5070/10000 training loss: 0.379191\n",
      "Batch 5080/10000 training loss: 0.303282\n",
      "Batch 5090/10000 training loss: 0.277222\n",
      "Batch 5100/10000 training loss: 0.274629\n",
      "Batch 5110/10000 training loss: 0.334077\n",
      "Batch 5120/10000 training loss: 0.335979\n",
      "Batch 5130/10000 training loss: 0.282867\n",
      "Batch 5140/10000 training loss: 0.253888\n",
      "Batch 5150/10000 training loss: 0.420256\n",
      "Batch 5160/10000 training loss: 0.379116\n",
      "Batch 5170/10000 training loss: 0.271931\n",
      "Batch 5180/10000 training loss: 0.292604\n",
      "Batch 5190/10000 training loss: 0.491604\n",
      "Batch 5200/10000 training loss: 0.338754\n",
      "Batch 5210/10000 training loss: 0.229672\n",
      "Batch 5220/10000 training loss: 0.194818\n",
      "Batch 5230/10000 training loss: 0.436659\n",
      "Batch 5240/10000 training loss: 0.384227\n",
      "Batch 5250/10000 training loss: 0.228050\n",
      "Batch 5260/10000 training loss: 0.270899\n",
      "Batch 5270/10000 training loss: 0.354069\n",
      "Batch 5280/10000 training loss: 0.392146\n",
      "Batch 5290/10000 training loss: 0.355610\n",
      "Batch 5300/10000 training loss: 0.297317\n",
      "Batch 5310/10000 training loss: 0.401144\n",
      "Batch 5320/10000 training loss: 0.226778\n",
      "Batch 5330/10000 training loss: 0.295802\n",
      "Batch 5340/10000 training loss: 0.186623\n",
      "Batch 5350/10000 training loss: 0.272486\n",
      "Batch 5360/10000 training loss: 0.407329\n",
      "Batch 5370/10000 training loss: 0.256156\n",
      "Batch 5380/10000 training loss: 0.343520\n",
      "Batch 5390/10000 training loss: 0.403117\n",
      "Batch 5400/10000 training loss: 0.268715\n",
      "Batch 5410/10000 training loss: 0.334447\n",
      "Batch 5420/10000 training loss: 0.379324\n",
      "Batch 5430/10000 training loss: 0.421274\n",
      "Batch 5440/10000 training loss: 0.419871\n",
      "Batch 5450/10000 training loss: 0.299602\n",
      "Batch 5460/10000 training loss: 0.246783\n",
      "Batch 5470/10000 training loss: 0.283846\n",
      "Batch 5480/10000 training loss: 0.383361\n",
      "Batch 5490/10000 training loss: 0.240767\n",
      "Batch 5500/10000 training loss: 0.266743\n",
      "Batch 5510/10000 training loss: 0.284641\n",
      "Batch 5520/10000 training loss: 0.248946\n",
      "Batch 5530/10000 training loss: 0.307526\n",
      "Batch 5540/10000 training loss: 0.309553\n",
      "Batch 5550/10000 training loss: 0.242766\n",
      "Batch 5560/10000 training loss: 0.248850\n",
      "Batch 5570/10000 training loss: 0.259432\n",
      "Batch 5580/10000 training loss: 0.281061\n",
      "Batch 5590/10000 training loss: 0.252524\n",
      "Batch 5600/10000 training loss: 0.289336\n",
      "Batch 5610/10000 training loss: 0.382656\n",
      "Batch 5620/10000 training loss: 0.265413\n",
      "Batch 5630/10000 training loss: 0.167647\n",
      "Batch 5640/10000 training loss: 0.274225\n",
      "Batch 5650/10000 training loss: 0.388061\n",
      "Batch 5660/10000 training loss: 0.246875\n",
      "Batch 5670/10000 training loss: 0.245839\n",
      "Batch 5680/10000 training loss: 0.259643\n",
      "Batch 5690/10000 training loss: 0.334948\n",
      "Batch 5700/10000 training loss: 0.384480\n",
      "Batch 5710/10000 training loss: 0.357265\n",
      "Batch 5720/10000 training loss: 0.229452\n",
      "Batch 5730/10000 training loss: 0.234988\n",
      "Batch 5740/10000 training loss: 0.225521\n",
      "Batch 5750/10000 training loss: 0.196136\n",
      "Batch 5760/10000 training loss: 0.152991\n",
      "Batch 5770/10000 training loss: 0.358892\n",
      "Batch 5780/10000 training loss: 0.397698\n",
      "Batch 5790/10000 training loss: 0.141805\n",
      "Batch 5800/10000 training loss: 0.287561\n",
      "Batch 5810/10000 training loss: 0.253562\n",
      "Batch 5820/10000 training loss: 0.299940\n",
      "Batch 5830/10000 training loss: 0.209379\n",
      "Batch 5840/10000 training loss: 0.262284\n",
      "Batch 5850/10000 training loss: 0.277426\n",
      "Batch 5860/10000 training loss: 0.233560\n",
      "Batch 5870/10000 training loss: 0.486407\n",
      "Batch 5880/10000 training loss: 0.289498\n",
      "Batch 5890/10000 training loss: 0.369943\n",
      "Batch 5900/10000 training loss: 0.355587\n",
      "Batch 5910/10000 training loss: 0.251911\n",
      "Batch 5920/10000 training loss: 0.270440\n",
      "Batch 5930/10000 training loss: 0.310407\n",
      "Batch 5940/10000 training loss: 0.324633\n",
      "Batch 5950/10000 training loss: 0.529593\n",
      "Batch 5960/10000 training loss: 0.376408\n",
      "Batch 5970/10000 training loss: 0.205299\n",
      "Batch 5980/10000 training loss: 0.414947\n",
      "Batch 5990/10000 training loss: 0.230299\n",
      "Batch 6000/10000 training loss: 0.321822\n",
      "Batch 6010/10000 training loss: 0.204225\n",
      "Batch 6020/10000 training loss: 0.230523\n",
      "Batch 6030/10000 training loss: 0.290725\n",
      "Batch 6040/10000 training loss: 0.260515\n",
      "Batch 6050/10000 training loss: 0.371091\n",
      "Batch 6060/10000 training loss: 0.347442\n",
      "Batch 6070/10000 training loss: 0.312868\n",
      "Batch 6080/10000 training loss: 0.302807\n",
      "Batch 6090/10000 training loss: 0.223122\n",
      "Batch 6100/10000 training loss: 0.351301\n",
      "Batch 6110/10000 training loss: 0.356234\n",
      "Batch 6120/10000 training loss: 0.263871\n",
      "Batch 6130/10000 training loss: 0.298295\n",
      "Batch 6140/10000 training loss: 0.212759\n",
      "Batch 6150/10000 training loss: 0.382911\n",
      "Batch 6160/10000 training loss: 0.364806\n",
      "Batch 6170/10000 training loss: 0.196943\n",
      "Batch 6180/10000 training loss: 0.370168\n",
      "Batch 6190/10000 training loss: 0.338249\n",
      "Batch 6200/10000 training loss: 0.333106\n",
      "Batch 6210/10000 training loss: 0.283292\n",
      "Batch 6220/10000 training loss: 0.362700\n",
      "Batch 6230/10000 training loss: 0.395871\n",
      "Batch 6240/10000 training loss: 0.206827\n",
      "Batch 6250/10000 training loss: 0.390794\n",
      "Batch 6260/10000 training loss: 0.291959\n",
      "Batch 6270/10000 training loss: 0.262595\n",
      "Batch 6280/10000 training loss: 0.239391\n",
      "Batch 6290/10000 training loss: 0.305942\n",
      "Batch 6300/10000 training loss: 0.309222\n",
      "Batch 6310/10000 training loss: 0.257744\n",
      "Batch 6320/10000 training loss: 0.235842\n",
      "Batch 6330/10000 training loss: 0.289433\n",
      "Batch 6340/10000 training loss: 0.338213\n",
      "Batch 6350/10000 training loss: 0.261778\n",
      "Batch 6360/10000 training loss: 0.216571\n",
      "Batch 6370/10000 training loss: 0.365136\n",
      "Batch 6380/10000 training loss: 0.247179\n",
      "Batch 6390/10000 training loss: 0.370567\n",
      "Batch 6400/10000 training loss: 0.156003\n",
      "Batch 6410/10000 training loss: 0.278009\n",
      "Batch 6420/10000 training loss: 0.292806\n",
      "Batch 6430/10000 training loss: 0.354029\n",
      "Batch 6440/10000 training loss: 0.264370\n",
      "Batch 6450/10000 training loss: 0.330142\n",
      "Batch 6460/10000 training loss: 0.201157\n",
      "Batch 6470/10000 training loss: 0.302788\n",
      "Batch 6480/10000 training loss: 0.265195\n",
      "Batch 6490/10000 training loss: 0.249863\n",
      "Batch 6500/10000 training loss: 0.240189\n",
      "Batch 6510/10000 training loss: 0.444439\n",
      "Batch 6520/10000 training loss: 0.176208\n",
      "Batch 6530/10000 training loss: 0.327404\n",
      "Batch 6540/10000 training loss: 0.336858\n",
      "Batch 6550/10000 training loss: 0.153180\n",
      "Batch 6560/10000 training loss: 0.371197\n",
      "Batch 6570/10000 training loss: 0.286455\n",
      "Batch 6580/10000 training loss: 0.263329\n",
      "Batch 6590/10000 training loss: 0.204501\n",
      "Batch 6600/10000 training loss: 0.136158\n",
      "Batch 6610/10000 training loss: 0.371259\n",
      "Batch 6620/10000 training loss: 0.381586\n",
      "Batch 6630/10000 training loss: 0.469724\n",
      "Batch 6640/10000 training loss: 0.224386\n",
      "Batch 6650/10000 training loss: 0.339148\n",
      "Batch 6660/10000 training loss: 0.215401\n",
      "Batch 6670/10000 training loss: 0.559673\n",
      "Batch 6680/10000 training loss: 0.323428\n",
      "Batch 6690/10000 training loss: 0.422841\n",
      "Batch 6700/10000 training loss: 0.183666\n",
      "Batch 6710/10000 training loss: 0.354894\n",
      "Batch 6720/10000 training loss: 0.269909\n",
      "Batch 6730/10000 training loss: 0.292559\n",
      "Batch 6740/10000 training loss: 0.344887\n",
      "Batch 6750/10000 training loss: 0.470030\n",
      "Batch 6760/10000 training loss: 0.222017\n",
      "Batch 6770/10000 training loss: 0.274538\n",
      "Batch 6780/10000 training loss: 0.337949\n",
      "Batch 6790/10000 training loss: 0.327827\n",
      "Batch 6800/10000 training loss: 0.447341\n",
      "Batch 6810/10000 training loss: 0.406343\n",
      "Batch 6820/10000 training loss: 0.198161\n",
      "Batch 6830/10000 training loss: 0.164932\n",
      "Batch 6840/10000 training loss: 0.321575\n",
      "Batch 6850/10000 training loss: 0.192220\n",
      "Batch 6860/10000 training loss: 0.389565\n",
      "Batch 6870/10000 training loss: 0.156772\n",
      "Batch 6880/10000 training loss: 0.393338\n",
      "Batch 6890/10000 training loss: 0.274007\n",
      "Batch 6900/10000 training loss: 0.356259\n",
      "Batch 6910/10000 training loss: 0.249764\n",
      "Batch 6920/10000 training loss: 0.280828\n",
      "Batch 6930/10000 training loss: 0.294545\n",
      "Batch 6940/10000 training loss: 0.266024\n",
      "Batch 6950/10000 training loss: 0.158970\n",
      "Batch 6960/10000 training loss: 0.192514\n",
      "Batch 6970/10000 training loss: 0.304978\n",
      "Batch 6980/10000 training loss: 0.509235\n",
      "Batch 6990/10000 training loss: 0.356676\n",
      "Batch 7000/10000 training loss: 0.328404\n",
      "Batch 7010/10000 training loss: 0.157699\n",
      "Batch 7020/10000 training loss: 0.302525\n",
      "Batch 7030/10000 training loss: 0.274887\n",
      "Batch 7040/10000 training loss: 0.234905\n",
      "Batch 7050/10000 training loss: 0.425688\n",
      "Batch 7060/10000 training loss: 0.318565\n",
      "Batch 7070/10000 training loss: 0.251972\n",
      "Batch 7080/10000 training loss: 0.284671\n",
      "Batch 7090/10000 training loss: 0.270356\n",
      "Batch 7100/10000 training loss: 0.351481\n",
      "Batch 7110/10000 training loss: 0.324327\n",
      "Batch 7120/10000 training loss: 0.186998\n",
      "Batch 7130/10000 training loss: 0.415292\n",
      "Batch 7140/10000 training loss: 0.424357\n",
      "Batch 7150/10000 training loss: 0.437235\n",
      "Batch 7160/10000 training loss: 0.300075\n",
      "Batch 7170/10000 training loss: 0.257194\n",
      "Batch 7180/10000 training loss: 0.377106\n",
      "Batch 7190/10000 training loss: 0.250840\n",
      "Batch 7200/10000 training loss: 0.312644\n",
      "Batch 7210/10000 training loss: 0.299789\n",
      "Batch 7220/10000 training loss: 0.270697\n",
      "Batch 7230/10000 training loss: 0.356132\n",
      "Batch 7240/10000 training loss: 0.260156\n",
      "Batch 7250/10000 training loss: 0.207193\n",
      "Batch 7260/10000 training loss: 0.257197\n",
      "Batch 7270/10000 training loss: 0.233332\n",
      "Batch 7280/10000 training loss: 0.294441\n",
      "Batch 7290/10000 training loss: 0.325966\n",
      "Batch 7300/10000 training loss: 0.328485\n",
      "Batch 7310/10000 training loss: 0.289057\n",
      "Batch 7320/10000 training loss: 0.431618\n",
      "Batch 7330/10000 training loss: 0.281320\n",
      "Batch 7340/10000 training loss: 0.468293\n",
      "Batch 7350/10000 training loss: 0.353535\n",
      "Batch 7360/10000 training loss: 0.345602\n",
      "Batch 7370/10000 training loss: 0.391201\n",
      "Batch 7380/10000 training loss: 0.300589\n",
      "Batch 7390/10000 training loss: 0.253017\n",
      "Batch 7400/10000 training loss: 0.389406\n",
      "Batch 7410/10000 training loss: 0.281141\n",
      "Batch 7420/10000 training loss: 0.198016\n",
      "Batch 7430/10000 training loss: 0.152927\n",
      "Batch 7440/10000 training loss: 0.273534\n",
      "Batch 7450/10000 training loss: 0.345555\n",
      "Batch 7460/10000 training loss: 0.474196\n",
      "Batch 7470/10000 training loss: 0.238742\n",
      "Batch 7480/10000 training loss: 0.253243\n",
      "Batch 7490/10000 training loss: 0.215700\n",
      "Batch 7500/10000 training loss: 0.275679\n",
      "Batch 7510/10000 training loss: 0.207205\n",
      "Batch 7520/10000 training loss: 0.260793\n",
      "Batch 7530/10000 training loss: 0.436878\n",
      "Batch 7540/10000 training loss: 0.228129\n",
      "Batch 7550/10000 training loss: 0.202615\n",
      "Batch 7560/10000 training loss: 0.269332\n",
      "Batch 7570/10000 training loss: 0.280392\n",
      "Batch 7580/10000 training loss: 0.165890\n",
      "Batch 7590/10000 training loss: 0.208976\n",
      "Batch 7600/10000 training loss: 0.274291\n",
      "Batch 7610/10000 training loss: 0.270657\n",
      "Batch 7620/10000 training loss: 0.237517\n",
      "Batch 7630/10000 training loss: 0.296062\n",
      "Batch 7640/10000 training loss: 0.170866\n",
      "Batch 7650/10000 training loss: 0.517804\n",
      "Batch 7660/10000 training loss: 0.156646\n",
      "Batch 7670/10000 training loss: 0.272082\n",
      "Batch 7680/10000 training loss: 0.231464\n",
      "Batch 7690/10000 training loss: 0.276969\n",
      "Batch 7700/10000 training loss: 0.315187\n",
      "Batch 7710/10000 training loss: 0.484668\n",
      "Batch 7720/10000 training loss: 0.370178\n",
      "Batch 7730/10000 training loss: 0.307851\n",
      "Batch 7740/10000 training loss: 0.228224\n",
      "Batch 7750/10000 training loss: 0.313630\n",
      "Batch 7760/10000 training loss: 0.231930\n",
      "Batch 7770/10000 training loss: 0.311937\n",
      "Batch 7780/10000 training loss: 0.282236\n",
      "Batch 7790/10000 training loss: 0.233641\n",
      "Batch 7800/10000 training loss: 0.240225\n",
      "Batch 7810/10000 training loss: 0.179441\n",
      "Batch 7820/10000 training loss: 0.255125\n",
      "Batch 7830/10000 training loss: 0.347284\n",
      "Batch 7840/10000 training loss: 0.269620\n",
      "Batch 7850/10000 training loss: 0.402209\n",
      "Batch 7860/10000 training loss: 0.365703\n",
      "Batch 7870/10000 training loss: 0.191698\n",
      "Batch 7880/10000 training loss: 0.281122\n",
      "Batch 7890/10000 training loss: 0.344406\n",
      "Batch 7900/10000 training loss: 0.245656\n",
      "Batch 7910/10000 training loss: 0.236461\n",
      "Batch 7920/10000 training loss: 0.272380\n",
      "Batch 7930/10000 training loss: 0.426641\n",
      "Batch 7940/10000 training loss: 0.229166\n",
      "Batch 7950/10000 training loss: 0.345439\n",
      "Batch 7960/10000 training loss: 0.227405\n",
      "Batch 7970/10000 training loss: 0.194149\n",
      "Batch 7980/10000 training loss: 0.190917\n",
      "Batch 7990/10000 training loss: 0.337144\n",
      "Batch 8000/10000 training loss: 0.308555\n",
      "Batch 8010/10000 training loss: 0.399453\n",
      "Batch 8020/10000 training loss: 0.295682\n",
      "Batch 8030/10000 training loss: 0.256448\n",
      "Batch 8040/10000 training loss: 0.233063\n",
      "Batch 8050/10000 training loss: 0.447772\n",
      "Batch 8060/10000 training loss: 0.303236\n",
      "Batch 8070/10000 training loss: 0.202494\n",
      "Batch 8080/10000 training loss: 0.248265\n",
      "Batch 8090/10000 training loss: 0.301368\n",
      "Batch 8100/10000 training loss: 0.688298\n",
      "Batch 8110/10000 training loss: 0.349445\n",
      "Batch 8120/10000 training loss: 0.218052\n",
      "Batch 8130/10000 training loss: 0.251499\n",
      "Batch 8140/10000 training loss: 0.215157\n",
      "Batch 8150/10000 training loss: 0.207846\n",
      "Batch 8160/10000 training loss: 0.362364\n",
      "Batch 8170/10000 training loss: 0.298604\n",
      "Batch 8180/10000 training loss: 0.326742\n",
      "Batch 8190/10000 training loss: 0.302089\n",
      "Batch 8200/10000 training loss: 0.424967\n",
      "Batch 8210/10000 training loss: 0.389493\n",
      "Batch 8220/10000 training loss: 0.432403\n",
      "Batch 8230/10000 training loss: 0.209645\n",
      "Batch 8240/10000 training loss: 0.159548\n",
      "Batch 8250/10000 training loss: 0.395172\n",
      "Batch 8260/10000 training loss: 0.374732\n",
      "Batch 8270/10000 training loss: 0.236544\n",
      "Batch 8280/10000 training loss: 0.396552\n",
      "Batch 8290/10000 training loss: 0.307390\n",
      "Batch 8300/10000 training loss: 0.294221\n",
      "Batch 8310/10000 training loss: 0.380927\n",
      "Batch 8320/10000 training loss: 0.251966\n",
      "Batch 8330/10000 training loss: 0.378771\n",
      "Batch 8340/10000 training loss: 0.218403\n",
      "Batch 8350/10000 training loss: 0.235247\n",
      "Batch 8360/10000 training loss: 0.372412\n",
      "Batch 8370/10000 training loss: 0.260507\n",
      "Batch 8380/10000 training loss: 0.312613\n",
      "Batch 8390/10000 training loss: 0.199335\n",
      "Batch 8400/10000 training loss: 0.422800\n",
      "Batch 8410/10000 training loss: 0.278265\n",
      "Batch 8420/10000 training loss: 0.328571\n",
      "Batch 8430/10000 training loss: 0.479154\n",
      "Batch 8440/10000 training loss: 0.342136\n",
      "Batch 8450/10000 training loss: 0.444343\n",
      "Batch 8460/10000 training loss: 0.243450\n",
      "Batch 8470/10000 training loss: 0.354107\n",
      "Batch 8480/10000 training loss: 0.287923\n",
      "Batch 8490/10000 training loss: 0.445494\n",
      "Batch 8500/10000 training loss: 0.215337\n",
      "Batch 8510/10000 training loss: 0.237927\n",
      "Batch 8520/10000 training loss: 0.236347\n",
      "Batch 8530/10000 training loss: 0.467378\n",
      "Batch 8540/10000 training loss: 0.241304\n",
      "Batch 8550/10000 training loss: 0.246478\n",
      "Batch 8560/10000 training loss: 0.154129\n",
      "Batch 8570/10000 training loss: 0.274928\n",
      "Batch 8580/10000 training loss: 0.266128\n",
      "Batch 8590/10000 training loss: 0.249083\n",
      "Batch 8600/10000 training loss: 0.252986\n",
      "Batch 8610/10000 training loss: 0.437123\n",
      "Batch 8620/10000 training loss: 0.298057\n",
      "Batch 8630/10000 training loss: 0.299692\n",
      "Batch 8640/10000 training loss: 0.294132\n",
      "Batch 8650/10000 training loss: 0.300156\n",
      "Batch 8660/10000 training loss: 0.258125\n",
      "Batch 8670/10000 training loss: 0.364694\n",
      "Batch 8680/10000 training loss: 0.324791\n",
      "Batch 8690/10000 training loss: 0.206922\n",
      "Batch 8700/10000 training loss: 0.250630\n",
      "Batch 8710/10000 training loss: 0.383552\n",
      "Batch 8720/10000 training loss: 0.181440\n",
      "Batch 8730/10000 training loss: 0.115892\n",
      "Batch 8740/10000 training loss: 0.438048\n",
      "Batch 8750/10000 training loss: 0.182186\n",
      "Batch 8760/10000 training loss: 0.236134\n",
      "Batch 8770/10000 training loss: 0.261056\n",
      "Batch 8780/10000 training loss: 0.218704\n",
      "Batch 8790/10000 training loss: 0.397070\n",
      "Batch 8800/10000 training loss: 0.259849\n",
      "Batch 8810/10000 training loss: 0.443492\n",
      "Batch 8820/10000 training loss: 0.157473\n",
      "Batch 8830/10000 training loss: 0.380718\n",
      "Batch 8840/10000 training loss: 0.147209\n",
      "Batch 8850/10000 training loss: 0.137465\n",
      "Batch 8860/10000 training loss: 0.376064\n",
      "Batch 8870/10000 training loss: 0.318647\n",
      "Batch 8880/10000 training loss: 0.295433\n",
      "Batch 8890/10000 training loss: 0.399026\n",
      "Batch 8900/10000 training loss: 0.399820\n",
      "Batch 8910/10000 training loss: 0.318883\n",
      "Batch 8920/10000 training loss: 0.216678\n",
      "Batch 8930/10000 training loss: 0.398944\n",
      "Batch 8940/10000 training loss: 0.337660\n",
      "Batch 8950/10000 training loss: 0.304649\n",
      "Batch 8960/10000 training loss: 0.299629\n",
      "Batch 8970/10000 training loss: 0.209714\n",
      "Batch 8980/10000 training loss: 0.214012\n",
      "Batch 8990/10000 training loss: 0.414859\n",
      "Batch 9000/10000 training loss: 0.172796\n",
      "Batch 9010/10000 training loss: 0.237648\n",
      "Batch 9020/10000 training loss: 0.243919\n",
      "Batch 9030/10000 training loss: 0.518692\n",
      "Batch 9040/10000 training loss: 0.343205\n",
      "Batch 9050/10000 training loss: 0.371272\n",
      "Batch 9060/10000 training loss: 0.327956\n",
      "Batch 9070/10000 training loss: 0.179897\n",
      "Batch 9080/10000 training loss: 0.216788\n",
      "Batch 9090/10000 training loss: 0.343650\n",
      "Batch 9100/10000 training loss: 0.428720\n",
      "Batch 9110/10000 training loss: 0.129821\n",
      "Batch 9120/10000 training loss: 0.300554\n",
      "Batch 9130/10000 training loss: 0.291699\n",
      "Batch 9140/10000 training loss: 0.184944\n",
      "Batch 9150/10000 training loss: 0.389793\n",
      "Batch 9160/10000 training loss: 0.306847\n",
      "Batch 9170/10000 training loss: 0.255502\n",
      "Batch 9180/10000 training loss: 0.434179\n",
      "Batch 9190/10000 training loss: 0.231853\n",
      "Batch 9200/10000 training loss: 0.321423\n",
      "Batch 9210/10000 training loss: 0.161540\n",
      "Batch 9220/10000 training loss: 0.228559\n",
      "Batch 9230/10000 training loss: 0.154340\n",
      "Batch 9240/10000 training loss: 0.390681\n",
      "Batch 9250/10000 training loss: 0.343813\n",
      "Batch 9260/10000 training loss: 0.270784\n",
      "Batch 9270/10000 training loss: 0.260001\n",
      "Batch 9280/10000 training loss: 0.371655\n",
      "Batch 9290/10000 training loss: 0.314528\n",
      "Batch 9300/10000 training loss: 0.233872\n",
      "Batch 9310/10000 training loss: 0.243542\n",
      "Batch 9320/10000 training loss: 0.291695\n",
      "Batch 9330/10000 training loss: 0.208540\n",
      "Batch 9340/10000 training loss: 0.211153\n",
      "Batch 9350/10000 training loss: 0.215885\n",
      "Batch 9360/10000 training loss: 0.154234\n",
      "Batch 9370/10000 training loss: 0.291072\n",
      "Batch 9380/10000 training loss: 0.264161\n",
      "Batch 9390/10000 training loss: 0.230996\n",
      "Batch 9400/10000 training loss: 0.566327\n",
      "Batch 9410/10000 training loss: 0.292572\n",
      "Batch 9420/10000 training loss: 0.260146\n",
      "Batch 9430/10000 training loss: 0.263215\n",
      "Batch 9440/10000 training loss: 0.224664\n",
      "Batch 9450/10000 training loss: 0.354531\n",
      "Batch 9460/10000 training loss: 0.364847\n",
      "Batch 9470/10000 training loss: 0.535299\n",
      "Batch 9480/10000 training loss: 0.356454\n",
      "Batch 9490/10000 training loss: 0.375768\n",
      "Batch 9500/10000 training loss: 0.302569\n",
      "Batch 9510/10000 training loss: 0.375156\n",
      "Batch 9520/10000 training loss: 0.200210\n",
      "Batch 9530/10000 training loss: 0.352345\n",
      "Batch 9540/10000 training loss: 0.264195\n",
      "Batch 9550/10000 training loss: 0.384894\n",
      "Batch 9560/10000 training loss: 0.230527\n",
      "Batch 9570/10000 training loss: 0.233863\n",
      "Batch 9580/10000 training loss: 0.243134\n",
      "Batch 9590/10000 training loss: 0.319353\n",
      "Batch 9600/10000 training loss: 0.455060\n",
      "Batch 9610/10000 training loss: 0.251132\n",
      "Batch 9620/10000 training loss: 0.139442\n",
      "Batch 9630/10000 training loss: 0.143375\n",
      "Batch 9640/10000 training loss: 0.316853\n",
      "Batch 9650/10000 training loss: 0.371440\n",
      "Batch 9660/10000 training loss: 0.243170\n",
      "Batch 9670/10000 training loss: 0.201910\n",
      "Batch 9680/10000 training loss: 0.170968\n",
      "Batch 9690/10000 training loss: 0.286478\n",
      "Batch 9700/10000 training loss: 0.286332\n",
      "Batch 9710/10000 training loss: 0.318611\n",
      "Batch 9720/10000 training loss: 0.315788\n",
      "Batch 9730/10000 training loss: 0.331798\n",
      "Batch 9740/10000 training loss: 0.398530\n",
      "Batch 9750/10000 training loss: 0.210854\n",
      "Batch 9760/10000 training loss: 0.245692\n",
      "Batch 9770/10000 training loss: 0.301656\n",
      "Batch 9780/10000 training loss: 0.220179\n",
      "Batch 9790/10000 training loss: 0.224098\n",
      "Batch 9800/10000 training loss: 0.264331\n",
      "Batch 9810/10000 training loss: 0.222484\n",
      "Batch 9820/10000 training loss: 0.238535\n",
      "Batch 9830/10000 training loss: 0.181199\n",
      "Batch 9840/10000 training loss: 0.289630\n",
      "Batch 9850/10000 training loss: 0.295567\n",
      "Batch 9860/10000 training loss: 0.170265\n",
      "Batch 9870/10000 training loss: 0.444334\n",
      "Batch 9880/10000 training loss: 0.377672\n",
      "Batch 9890/10000 training loss: 0.275085\n",
      "Batch 9900/10000 training loss: 0.291807\n",
      "Batch 9910/10000 training loss: 0.256602\n",
      "Batch 9920/10000 training loss: 0.263952\n",
      "Batch 9930/10000 training loss: 0.301657\n",
      "Batch 9940/10000 training loss: 0.336404\n",
      "Batch 9950/10000 training loss: 0.216354\n",
      "Batch 9960/10000 training loss: 0.363309\n",
      "Batch 9970/10000 training loss: 0.192827\n",
      "Batch 9980/10000 training loss: 0.309406\n",
      "Batch 9990/10000 training loss: 0.287473\n",
      "Batch 10000/10000 training loss: 0.275481\n",
      "Total time training 20 epochs: 17.8850769997 s\n"
     ]
    }
   ],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# specify number of epochs to run through whole dataset\n",
    "num_epochs = 20 # approx xxx s / epoch on laptop (macbook pro 13\" i7 end 2011 w/ 8GB RAM) w/ batch_size = 100\n",
    "# compute total amount of batches to be run\n",
    "train_size = 50000\n",
    "# specify batch_size \n",
    "batch_size = 100\n",
    "num_batches = int((train_size / batch_size) * num_epochs)\n",
    "# print loss after this amount of batches\n",
    "print_every = 10\n",
    "\n",
    "tr_losses = []\n",
    "print('Training...')\n",
    "beg_t = timeit.default_timer()\n",
    "# Run the training iterations\n",
    "for curr_batch in range(num_batches):\n",
    "    # get the batches of training images (injected to x placeholder) and labels (injected to y_ placeholder)\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # run model update (learning stage over a batch of samples)\n",
    "    tr_loss , _= sess.run([loss, train_step], feed_dict={x: batch_x, y_:batch_y})\n",
    "    tr_losses.append(tr_loss)\n",
    "    if (curr_batch + 1) % print_every == 0:\n",
    "        print('Batch {}/{} training loss: {:.6f}'.format(curr_batch + 1, num_batches, tr_loss))\n",
    "end_t = timeit.default_timer()\n",
    "print('Total time training {} epochs: {} s'.format(num_epochs, end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f6e7c6256d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAEKCAYAAAAYW4wpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFPH/CvAnPbte+zwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e7e585f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.semilogy(tr_losses)\n",
    "plt.xlabel('Batch index')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in classifying mnist digits: 0.921800136566\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "# cache the real test images and labels\n",
    "te_x_batch = mnist.test.images\n",
    "te_y_batch = mnist.test.labels\n",
    "# define the accuracy computation op\n",
    "# NOTE: must take the argmax of prediction at softmax output to get predicted class with more probability\n",
    "# Argmax also done in y_ to get index out of one-hot vector\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy in classifying '\n",
    "      'mnist digits: {}'.format(sess.run(accuracy, feed_dict={x:te_x_batch, y_:te_y_batch})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
