{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifying with artificial neurons\n",
    "\n",
    "## Introduction: The perceptron\n",
    "\n",
    "The first example of this notebook is a binary classifier by means of the Logistic Regression operation. This model is also called Perceptron or Artificial Neuron, depicted in the figure below.\n",
    "\n",
    "![perceptron](assets/perceptron.png)\n",
    "\n",
    "There we have a set of inputs {x1, x2, x3}, a set of weights {w1, w2, w3}, a bias factor {b} and an activation function {f}, and the following operations are applied:\n",
    "\n",
    "$$y = f(\\sum_{m=1}^{M} x_i * w_i + b)$$\n",
    "\n",
    "To actually make it a binary classifier we must place a specific type of activation function called Sigmoid: \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{(1 + {e}^{-z})}$$ where $$z = x * w + b$$\n",
    "\n",
    "The Sigmoid shape is depicted in the figure below:\n",
    "\n",
    "![sigmoid](assets/sigmoid.png)\n",
    "\n",
    "We can see how, depending on the weights and biases (in the depicted figure all sigmoids have scalar values, so only one input x1 would be injected) there is a ridge bounding the outputs between (0, 1), which can be interpreted as a probability output depending on the inputs that activate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1: It is a number or is it noise?\n",
    "\n",
    "In this example we will classify whether an image is noise or a MNIST digit:\n",
    "* MNIST dataset contains images of 10 handwritten digit classes {0, 1, 2, 3, 4, ..., 9}. Each class contains 6.000 images of 28x28 pixels.\n",
    "\n",
    "We will use 50.000 images for training and 10.000 for testing our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import print utility and timer\n",
    "from __future__ import print_function\n",
    "import timeit\n",
    "# First import tensorflow and the data reader\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "# numpy for matrix utilities\n",
    "import numpy as np\n",
    "from utils import plot_samples\n",
    "# import plot utilities\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define IMG dimensions\n",
    "IMG_SIZE=28\n",
    "# Import mnist data\n",
    "mnist = input_data.read_data_sets('data/mnist/', one_hot=True)\n",
    "# Make the random images generator (28x28 withdrawn from a random uniform distribution)\n",
    "def make_random_batch(batch_size):\n",
    "    # Generate batch_size images of size image_size x image_size\n",
    "    rimg = np.random.uniform(low=0., high=1., size=(batch_size, IMG_SIZE * IMG_SIZE))\n",
    "    return rimg\n",
    "# Define num of pixels that will be input to models\n",
    "unrolled_size = IMG_SIZE * IMG_SIZE\n",
    "print(\"Computed unrolled size: \", unrolled_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize some MNIST samples\n",
    "batch_x_real, _ = mnist.train.next_batch(100)\n",
    "batch_x_real = batch_x_real.reshape((100, 28, 28))\n",
    "plot_samples(batch_x_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Define the model operation of logistic regression equation and the placeholder to insert groundtruth binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare the model, recall logistic regression equation\n",
    "# Define weights matrix (from unrolled_size inputs to 1 classification output (noise(0)/mnist(1)))\n",
    "W = tf.Variable(tf.zeros([unrolled_size, 1]))\n",
    "# the bias is summing just a scalar output, so dimension 1\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "# define an input placeholder to inject the vectorized images\n",
    "# None indicates we don't know batch_size yet, will be specified when running the training\n",
    "x = tf.placeholder(tf.float32, [None, unrolled_size]) \n",
    "# TODO: Logistic Regression equation implementation\n",
    "# y = ...\n",
    "\n",
    "# apply sigmoid to get final predictions\n",
    "out = tf.sigmoid(y)\n",
    "\n",
    "# TODO: Now we define the placeholder to place the flag (0 or 1) as output examples\n",
    "# y_ = ...\n",
    "\n",
    "# Now call the sigmoid cross entropy with logits to compute the loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "# define the gradients update operation with learning rate of 0.05\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize the TensorFlow session to run the operations Graph \"on the fly\" (not usual in production code)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# specify number of epochs to run through whole dataset\n",
    "num_epochs = 1 # approx 55 s / epoch on laptop (macbook pro 13\" i7 end 2011 w/ 8GB RAM) w/ batch_size = 10\n",
    "# compute total amount of batches to be run\n",
    "train_size = 50000\n",
    "num_batches = int(train_size * num_epochs)\n",
    "# specify batch_size \n",
    "batch_size = 10\n",
    "# print loss after this amount of batches\n",
    "print_every = 10000\n",
    "tr_losses = []\n",
    "\n",
    "print('Training...')\n",
    "beg_t = timeit.default_timer()\n",
    "# Run the training iterations\n",
    "for curr_batch in range(num_batches):\n",
    "    # get the batch of training images (to be injected to x placeholder)\n",
    "    batch_x_real, _ = mnist.train.next_batch(batch_size)\n",
    "    # create the batch of labels (to be injected to y_ placeholder)\n",
    "    batch_y_real = np.ones((batch_size, 1))\n",
    "    # generate the batch of random images (to be injected to x placeholder)\n",
    "    batch_x_random = make_random_batch(batch_size)\n",
    "    # create the batch of 0 labels (to be injectd to y_ placeholder)\n",
    "    batch_y_random = np.zeros((batch_size, 1))\n",
    "    # merge both batches into one and run update\n",
    "    batch_x = np.concatenate((batch_x_real, batch_x_random), axis=0)\n",
    "    batch_y = np.concatenate((batch_y_real, batch_y_random), axis=0)\n",
    "    # run model update (learning stage over a batch of samples)\n",
    "    tr_loss , _= sess.run([loss, train_step], feed_dict={x: batch_x, y_:batch_y})\n",
    "    tr_losses.append(tr_loss)\n",
    "    if (curr_batch + 1) % print_every == 0:\n",
    "        print('Batch {}/{} training loss: {:.6f}'.format(curr_batch + 1, num_batches, tr_loss))\n",
    "end_t = timeit.default_timer()\n",
    "print('Total time training {} epochs: {} s'.format(num_epochs, end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.semilogy(tr_losses)\n",
    "plt.xlabel('Batch index')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Call the right test op to evaluate the performance of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test trained model\n",
    "# generate the random test images\n",
    "te_x_random = make_random_batch(10000)\n",
    "te_y_random = np.zeros((10000, 1))\n",
    "# cache the real test images\n",
    "te_x_real = mnist.test.images\n",
    "te_y_real = np.ones((10000, 1))\n",
    "# total test batches\n",
    "te_x_batch = np.concatenate((te_x_random, te_x_real), axis=0)\n",
    "te_y_batch = np.concatenate((te_y_random, te_y_real), axis=0)\n",
    "# define the accuracy computation op\n",
    "# NOTE: the sigmoid output is rounded so that if out >= 0.5 --> predicts 1, otherwise predicts 0\n",
    "correct_prediction = tf.equal(tf.round(out), y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# TODO: print Accuracy computation running the accuracy op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2: What number is it? Scaling up to multiple classes\n",
    "Now that a binary classification task has been solved we will see its natural extension: a multiclass classifier.\n",
    "This can be done by means of a softmax layer. The softmax layer is a parallel arrangement of sigmoidal neurons (*Output Layer* in the image below), where every neuron indicates the amount of probability that the input features (*Input Layer* in the image below) belong to a certain class.\n",
    "\n",
    "![softmax](assets/softmax_img.png)\n",
    "\n",
    "\n",
    "As it is a probability distribution between the possible classes, all of them must sum up to 1. So the softmax formulation is the following one:\n",
    "\n",
    "$$y = \\frac{e^{x^T * w_k}}{\\sum_{n=1}^{N} e^{x^T * w_n}}$$\n",
    "\n",
    "where x and w are vectors representing inputs $x$ and k-th layer weights $w_k$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initialize the TensorFlow session to run the operations Graph \"on the fly\" (not usual in production code)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First, define the new weights and biases to express the multiple output neurons\n",
    "# TODO: Define weights matrix (from unrolled_size inputs to 10 classification outputs (10 MNIST digits)\n",
    "# W = ...\n",
    "# TODO: define the biases\n",
    "# b = ...\n",
    "# TODO: define an input placeholder to inject the vectorized images\n",
    "# x = ...\n",
    "# TODO: equation implementation\n",
    "# y = ...\n",
    "# apply sigmoid to get final predictions\n",
    "out = tf.nn.softmax(y)\n",
    "\n",
    "# TODO: Now we define the placeholder to place the classes\n",
    "# y_ = ...\n",
    "\n",
    "# TODO: Now call the softmax cross entropy with logits to compute the loss function\n",
    "# loss = ...\n",
    "\n",
    "# TODO: define the gradients update operation with learning rate of 0.05\n",
    "# train_step = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# specify number of epochs to run through whole dataset\n",
    "num_epochs = 3 # approx xxx s / epoch on laptop (macbook pro 13\" i7 end 2011 w/ 8GB RAM) w/ batch_size = 100\n",
    "# compute total amount of batches to be run\n",
    "train_size = 50000\n",
    "num_batches = int(train_size * num_epochs)\n",
    "# specify batch_size \n",
    "batch_size = 100\n",
    "# print loss after this amount of batches\n",
    "print_every = 10000\n",
    "\n",
    "tr_losses = []\n",
    "print('Training...')\n",
    "beg_t = timeit.default_timer()\n",
    "# Run the training iterations\n",
    "for curr_batch in range(num_batches):\n",
    "    # get the batches of training images (injected to x placeholder) and labels (injected to y_ placeholder)\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # run model update (learning stage over a batch of samples)\n",
    "    tr_loss , _= sess.run([loss, train_step], feed_dict={x: batch_x, y_:batch_y})\n",
    "    tr_losses.append(tr_loss)\n",
    "    if (curr_batch + 1) % print_every == 0:\n",
    "        print('Batch {}/{} training loss: {:.6f}'.format(curr_batch + 1, num_batches, tr_loss))\n",
    "end_t = timeit.default_timer()\n",
    "print('Total time training {} epochs: {} s'.format(num_epochs, end_t - beg_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.semilogy(tr_losses)\n",
    "plt.xlabel('Batch index')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test trained model\n",
    "# cache the real test images and labels\n",
    "te_x_batch = mnist.test.images\n",
    "te_y_batch = mnist.test.labels\n",
    "# define the accuracy computation op\n",
    "# NOTE: must take the argmax of prediction at softmax output to get predicted class with more probability\n",
    "# Argmax also done in y_ to get index out of one-hot vector\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy in classifying '\n",
    "      'mnist digits: {}'.format(sess.run(accuracy, feed_dict={x:te_x_batch, y_:te_y_batch})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
